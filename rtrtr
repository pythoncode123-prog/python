#!/usr/bin/env python3
import os
import sys
import logging
from datetime import datetime

# Add the project root to the Python path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.workflow import run_workflow

# Configuration 
CONFIG_FILE = 'config/config.ini'
QUERY_FILE = 'config/query.sql'
OUTPUT_CSV = 'data.csv'
EXECUTION_TIMESTAMP = datetime.strptime('2025-05-20 23:36:40', '%Y-%m-%d %H:%M:%S')
EXECUTION_USER = 'satish537'

def setup_logging():
    """Set up logging configuration."""
    log_file = f"workflow_{EXECUTION_TIMESTAMP.strftime('%Y%m%d_%H%M%S')}.log"
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )

def main():
    """Main entry point."""
    # Setup logging
    setup_logging()
    
    # Create config directory if it doesn't exist
    os.makedirs('config', exist_ok=True)
    
    # Check if config files exist
    if not os.path.exists(CONFIG_FILE):
        logging.error(f"Config file {CONFIG_FILE} not found.")
        return 1
        
    if not os.path.exists(QUERY_FILE):
        logging.error(f"Query file {QUERY_FILE} not found.")
        return 1
    
    # Run the workflow
    return run_workflow(
        CONFIG_FILE,
        QUERY_FILE,
        OUTPUT_CSV,
        EXECUTION_TIMESTAMP,
        EXECUTION_USER
    )

if __name__ == "__main__":
    sys.exit(main())



------
import logging
from datetime import datetime
import os
from lib.db_utils import sql_to_csv
from lib.csv_processor import CSVProcessor
from lib.confluence_publisher import publish_to_confluence

def run_workflow(config_file, query_file, output_csv, execution_timestamp, execution_user):
    """Run the complete workflow."""
    print(f"Starting workflow at {datetime.now()}")
    
    # Step 1: SQL to CSV
    print("Step 1: SQL to CSV...")
    sql_result = sql_to_csv(config_file, query_file, output_csv)
    if not sql_result:
        print("ERROR: SQL to CSV process failed. Check log for details.")
        return 1
    print("SQL to CSV completed successfully.")
    
    # Step 2: CSV Processing
    print("\nStep 2: CSV Processing...")
    processor = CSVProcessor(execution_timestamp, execution_user)
    process_result = processor.process_all_files(output_csv)
    if not process_result:
        print("ERROR: CSV processing failed. Check log for details.")
        return 2
    print("CSV Processing completed successfully.")
    
    # Step 3: Confluence Publishing
    print("\nStep 3: Confluence Publishing...")
    confluence_result = publish_to_confluence()
    if not confluence_result:
        print("ERROR: Confluence publishing failed. Check log for details.")
        return 3
    
    print("\nComplete workflow executed successfully!")
    return 0


---

import os
import csv
from datetime import datetime
import logging
from typing import Dict, List, Tuple, Optional

class CSVProcessor:
    """Class to handle CSV file processing and data aggregation."""

    DATE_FORMATS = [
        '%Y-%m-%d %H:%M:%S', '%Y-%m-%d',
        '%d-%m-%Y', '%Y/%m/%d', '%d/%m/%Y',
        '%d/%m/%y', '%m/%d/%Y', '%m/%d/%y',
        '%Y.%m.%d', '%d.%m.%Y', '%d.%m.%y',
        '%m.%d.%Y', '%m.%d.%y'
    ]

    def __init__(self, execution_timestamp, execution_user):
        """Initialize the CSVProcessor."""
        self.execution_timestamp = execution_timestamp
        self.execution_user = execution_user
        self.summary_data: Dict[Tuple, int] = {}
        self.summary_data_region: Dict[Tuple, int] = {}

    @staticmethod
    def get_csv_files(directory: str) -> List[str]:
        """
        Get all CSV files in the specified directory.
        """
        return [file for file in os.listdir(directory) if file.endswith('.csv')]

    @staticmethod
    def parse_filename(filename: str) -> Tuple[Optional[str], Optional[str]]:
        """
        Extract region and environment from filename.
        """
        parts = filename.split('-')
        if len(parts) >= 2:
            return parts[0], parts[1]
        return None, None

    def parse_date(self, net_date: str) -> Optional[datetime]:
        """
        Parse date string using multiple formats.
        """
        for fmt in self.DATE_FORMATS:
            try:
                return datetime.strptime(net_date, fmt)
            except ValueError:
                continue
        return None

    def process_line(self, line: List[str], region: str, env: str):
        """
        Process a single line of CSV data.
        """
        if not line or len(line) < 3:
            return

        date_obj = self.parse_date(line[0])
        if date_obj is None:
            logging.warning(f"Could not parse date: {line[0]}")
            return

        try:
            date = date_obj.strftime('%Y-%m-%d')
            ctm_host_name = line[1].strip()
            
            # Try to parse the fvalue field as the jobs count
            try:
                jobs = int(line[2])
            except (ValueError, IndexError):
                # Fallback for data.csv format from SQL query
                if len(line) >= 4:
                    jobs = 1  # Each row counts as one job
                else:
                    logging.error(f"Cannot find job count in line: {line}")
                    return

            # Update summary dictionaries
            key = (region, env, date, ctm_host_name)
            key_region = (region, env, date)

            self.summary_data[key] = self.summary_data.get(key, 0) + jobs
            self.summary_data_region[key_region] = self.summary_data_region.get(key_region, 0) + jobs

        except Exception as e:
            logging.error(f"Error processing line: {line}: {str(e)}")

    def process_csv_file(self, file: str, default_csv_name: str) -> None:
        """
        Process a single CSV file.
        """
        logging.info(f"Processing file: {file}")
        try:
            with open(file, 'r', encoding='utf-8') as csv_file:
                reader = csv.reader(csv_file)
                lines = list(reader)

            if not lines:
                logging.warning(f"Empty file: {file}")
                return

            # Determine file format - from SQL or already processed
            # For SQL output (data.csv), we get region/env from filename
            # Otherwise use the original region/env logic
            
            filename = os.path.basename(file)
            if filename == default_csv_name:
                # For SQL output, assign default region
                region = "NA"
                env = "PROD"
                
                # Skip header
                if lines[0][0] == 'NET_DATE':
                    lines.pop(0)
                
            else:
                # For other files, extract region/env from filename
                filename = os.path.splitext(os.path.basename(file))[0]
                region, env = self.parse_filename(filename)
                
                if not (region and env):
                    logging.warning(f"Invalid filename format: {file}")
                    return
                    
                # Skip header if present
                if lines[0][0] == 'NET_DATE':
                    lines.pop(0)

            for line in lines:
                try:
                    self.process_line(line, region, env)
                except Exception as e:
                    logging.error(f"Error processing line in {file}: {str(e)}")

        except Exception as e:
            logging.error(f"Error processing file {file}: {str(e)}")

    def write_summary_to_csv(self, filename: str, headers: List[str], data: Dict[Tuple, int]):
        """
        Write summarized data to CSV file.
        """
        try:
            with open(filename, 'w', newline='') as csv_file:
                writer = csv.writer(csv_file)
                writer.writerow(headers + ['EXECUTION_TIMESTAMP', 'EXECUTION_USER'])

                for key, total_jobs in data.items():
                    writer.writerow(list(key) + [total_jobs, self.execution_timestamp.strftime('%Y-%m-%d %H:%M:%S'), self.execution_user])
            logging.info(f"Successfully wrote summary to {filename}")

        except Exception as e:
            logging.error(f"Error writing to {filename}: {str(e)}")

    def process_all_files(self, input_file=None, default_csv_name='data.csv') -> bool:
        """
        Process CSV files and generate summary reports.
        If input_file is provided, process only that file.
        """
        logging.info("Starting CSV processing")
        try:
            if input_file:
                if not os.path.exists(input_file):
                    logging.error(f"Input file {input_file} not found")
                    return False
                files_to_process = [input_file]
            else:
                files_to_process = [f for f in self.get_csv_files('.') if f != 'task_usage_report.csv' and f != 'task_usage_report_by_region.csv']
            
            if not files_to_process:
                logging.warning("No CSV files found to process")
                return False

            for file in files_to_process:
                self.process_csv_file(file, default_csv_name)

            # Write summary reports
            self.write_summary_to_csv(
                "task_usage_report.csv",
                ["REGION", "ENV", "DATE", "CTM_HOST_NAME", "TOTAL_JOBS"],
                self.summary_data
            )

            self.write_summary_to_csv(
                "task_usage_report_by_region.csv",
                ["REGION", "ENV", "DATE", "TOTAL_JOBS"],
                self.summary_data_region
            )
            
            logging.info("CSV processing completed successfully")
            return True
        except Exception as e:
            logging.error(f"Error in CSV processing: {str(e)}")
            return False


----


import pyodbc
import csv
import configparser
import logging
from datetime import datetime
import os

def load_config(config_path):
    """Load database configuration from file."""
    config = configparser.ConfigParser()
    config.read(config_path)
    db_config = config['DATABASE']
    return {
        'host': db_config['DBHost'],
        'port': db_config['Port'],
        'service': db_config['ServiceName'],
        'schema': db_config['SchemaName'],
        'user': db_config['Username'],
        'password': db_config['Password']
    }

def connect_sql_server(cfg):
    """Connect to SQL server using configuration."""
    conn_str = (
        f"DRIVER={{ODBC Driver 17 for SQL Server}};"
        f"SERVER={cfg['host']},{cfg['port']};"
        f"DATABASE={cfg['service']};"
        f"UID={cfg['user']};"
        f"PWD={cfg['password']}"
    )
    return pyodbc.connect(conn_str)

def execute_query_and_write_csv(conn, query, csv_path):
    """Execute query and write results to CSV file."""
    cursor = conn.cursor()
    cursor.execute(query)
    columns = [column[0] for column in cursor.description]
    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(columns)
        for row in cursor:
            writer.writerow(row)
    cursor.close()
    logging.info(f"SQL query results exported to {csv_path}")

def sql_to_csv(config_file, query_file, output_csv, execution_timestamp=None):
    """Execute SQL and export to CSV."""
    logging.info("Starting SQL to CSV process")
    try:
        # Check query file exists
        if not os.path.exists(query_file):
            logging.error(f"Query file '{query_file}' not found")
            return False

        cfg = load_config(config_file)
        with open(query_file, 'r', encoding='utf-8') as f:
            query = f.read()
        query = query.replace('<EM_schema>', cfg['schema'])
        
        conn = connect_sql_server(cfg)
        execute_query_and_write_csv(conn, query, output_csv)
        conn.close()
        logging.info("SQL to CSV process completed successfully")
        return True
    except Exception as e:
        logging.error(f"Error in SQL to CSV process: {str(e)}")
        return False



---

[DATABASE]
DBHost=your_db_host
Port=1433
ServiceName=your_db_name
SchemaName=your_schema
Username=your_username
Password=your_password

--

ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS';
SELECT TO_CHAR(net_report.net_date, 'YYYY-MM-DD HH24:MI:SS') AS NET_DATE,
       net_report.ctm_host_name,
       net_report_data.fvalue,
       net_report_data.job_id
FROM <EM_schema>.net_report,
     <EM_schema>.net_report_data
WHERE net_report.report_id=net_report_data.report_id
  AND net_report_data.fname='NODE_ID'
  AND net_report.net_date BETWEEN '2024-09-01 00:00:00' AND '2024-09-30 23:59:00'
ORDER BY net_report.net_date;
