#!/usr/bin/env python3
"""
Main workflow launcher - Enhanced with CSV auto-discovery.
FIXED: Properly handle YTD mode for CSV countries
"""

import os
import sys
import json
import logging
import re
import calendar
from datetime import datetime, timedelta
import argparse
import tempfile
import shutil
import glob
import pandas as pd

# Make project root importable
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.append(PROJECT_ROOT)

from src.workflow import run_workflow  # noqa: E402
from lib.secure_config import SecureConfig  # noqa: E402
from lib.csv_processor import CSVProcessor  # noqa: E402
from lib.confluence_publisher import publish_to_confluence  # noqa: E402

# [Keep all the constants and helper functions the same until process_csv_file_countries]

# ---------------------------------------------------------------------------
# FIXED: CSV Processing Functions with YTD support
# ---------------------------------------------------------------------------

def process_csv_file_countries(output_dir, target_month, test_mode=False, is_daily_mode=False):
    """
    Process countries that provide CSV files directly using auto-discovery.
    FIXED: Support YTD mode for daily runs
    
    Args:
        output_dir: Output directory for processed files
        target_month: Target month in YYYYMM format
        test_mode: If True, create test data
        is_daily_mode: If True, process as YTD; if False, process as monthly
    """
    # Load config to get CSV path
    try:
        with open('config/config.json', 'r') as f:
            config = json.load(f)
    except Exception as e:
        logging.warning(f"Could not load config for CSV countries: {e}")
        return []
    
    csv_path = config.get('CSV_COUNTRIES_PATH')
    if not csv_path:
        logging.info("No CSV_COUNTRIES_PATH configured, skipping CSV file countries")
        return []
    
    # Auto-discover CSV countries
    csv_countries = auto_discover_csv_countries(csv_path, target_month)
    
    if not csv_countries:
        logging.info(f"No CSV countries found for month {target_month}")
        return []
    
    logging.info(f"Processing {len(csv_countries)} auto-discovered CSV countries")
    logging.info(f"Mode: {'YTD/Daily' if is_daily_mode else 'Monthly'}")
    
    results = []
    
    for csv_country in csv_countries:
        country_name = csv_country['name']
        country_code = csv_country['country_code']
        source_file_path = csv_country['file_path']
        
        # Output file in our processing directory
        output_file = os.path.join(output_dir, f"data_{country_name.lower()}.csv")
        
        logging.info(f"Processing CSV country: {country_name} from {source_file_path}")
        
        if test_mode:
            # Create dummy test data for CSV countries
            create_test_csv_country_data(output_file, country_name, target_month)
            results.append({
                'name': country_name,
                'csv_file': output_file,
                'success': True,
                'source': 'csv_file'
            })
            logging.info(f"Created test data for CSV country: {country_name}")
            continue
        
        try:
            # Process the actual CSV file with daily mode flag
            success = process_single_csv_country_file(
                source_file_path, output_file, country_name, target_month,
                is_daily_mode=is_daily_mode  # Pass the flag here!
            )
            
            results.append({
                'name': country_name,
                'csv_file': output_file,
                'success': success,
                'source': 'csv_file'
            })
            
            if success:
                logging.info(f"Successfully processed CSV country: {country_name}")
            else:
                logging.error(f"Failed to process CSV country: {country_name}")
                
        except Exception as e:
            logging.error(f"Error processing CSV country {country_name}: {e}")
            results.append({
                'name': country_name,
                'csv_file': output_file,
                'success': False,
                'source': 'csv_file'
            })
    
    return results


def process_single_csv_country_file(source_file, output_file, country_name, target_month, is_daily_mode=False):
    """
    Process a single CSV file from a country and convert it to our standard format.
    FIXED: Support YTD extraction for daily mode
    
    Args:
        source_file: Path to source CSV file
        output_file: Path to output CSV file
        country_name: Name of the country
        target_month: Target month in YYYYMM format
        is_daily_mode: If True, extract YTD data; if False, extract only target month
    """
    try:
        # Read the source CSV
        df = pd.read_csv(source_file)
        
        logging.info(f"Loaded CSV for {country_name}: {len(df)} rows")
        logging.info(f"Columns: {list(df.columns)}")
        logging.info(f"Processing mode: {'YTD' if is_daily_mode else 'Monthly'}")
        
        # Log first few rows to understand the data
        logging.info(f"First 5 rows of raw data:")
        for i in range(min(5, len(df))):
            logging.info(f"  Row {i}: {df.iloc[i].to_dict()}")
        
        # Handle different possible column names for jobs
        jobs_column = None
        for col in ['JOBS', 'JOBS_PER_DAY', 'TOTAL_JOBS', 'JOB_COUNT', 'FVALUE']:
            if col in df.columns:
                jobs_column = col
                break
        
        if jobs_column is None:
            # If no jobs column found, assume the last numeric column
            numeric_cols = df.select_dtypes(include=['number']).columns
            if len(numeric_cols) > 0:
                jobs_column = numeric_cols[-1]
                logging.warning(f"No standard jobs column found, using: {jobs_column}")
            else:
                logging.error(f"No numeric column found for jobs in {country_name}")
                return False
        
        logging.info(f"Using jobs column: {jobs_column}")
        
        # Handle date column
        date_column = None
        for col in ['NET_DATE', 'DATE', 'DAY', 'REPORT_DATE']:
            if col in df.columns:
                date_column = col
                break
        
        if date_column is None:
            logging.error(f"No date column found in {country_name}")
            return False
        
        logging.info(f"Using date column: {date_column}")
        
        # Convert date column to datetime - handle various date formats
        date_formats = [
            '%d/%m/%Y %H:%M',     # 01/08/2025 08:00
            '%d/%m/%Y %H:%M:%S',  # 01/08/2025 08:00:00
            '%Y-%m-%d %H:%M:%S',  # 2025-08-01 08:00:00
            '%Y-%m-%d',           # 2025-08-01
            '%m/%d/%Y %H:%M',     # 08/01/2025 08:00 (US format)
            '%m/%d/%Y %H:%M:%S',  # 08/01/2025 08:00:00 (US format)
        ]
        
        # Try to parse dates with different formats
        date_parsed = False
        for date_format in date_formats:
            try:
                df[date_column] = pd.to_datetime(df[date_column], format=date_format)
                date_parsed = True
                logging.info(f"Successfully parsed dates using format: {date_format}")
                break
            except:
                continue
        
        # If no format worked, try pandas auto-detection
        if not date_parsed:
            try:
                df[date_column] = pd.to_datetime(df[date_column], dayfirst=True)
                date_parsed = True
                logging.info("Successfully parsed dates using pandas auto-detection with dayfirst=True")
            except:
                try:
                    df[date_column] = pd.to_datetime(df[date_column])
                    date_parsed = True
                    logging.info("Successfully parsed dates using pandas auto-detection")
                except Exception as e:
                    logging.error(f"Could not parse dates: {e}")
                    return False
        
        # Remove any rows where date parsing failed
        df = df[pd.notna(df[date_column])].copy()
        logging.info(f"After date parsing, {len(df)} valid rows remain")
        
        if df.empty:
            logging.error(f"No valid dates found after parsing in {country_name}")
            return False
        
        # Extract just the date part for grouping
        df['DATE_ONLY'] = df[date_column].dt.date
        
        # Log date range in the data
        min_date = df[date_column].min()
        max_date = df[date_column].max()
        logging.info(f"Date range in file: {min_date} to {max_date}")
        
        # Filter data based on mode
        if is_daily_mode:
            # For daily/YTD mode: get all data from current year
            current_year = datetime.now().year
            df_filtered = df[df[date_column].dt.year == current_year].copy()
            logging.info(f"YTD mode: filtered to {len(df_filtered)} rows for entire year {current_year}")
            
            if df_filtered.empty:
                logging.warning(f"No data found for year {current_year} in {country_name}, using all data")
                df_filtered = df.copy()
        else:
            # For monthly mode: filter to specific month
            if target_month:
                target_year = int(target_month[:4])
                target_month_num = int(target_month[4:6])
                
                df_filtered = df[
                    (df[date_column].dt.year == target_year) & 
                    (df[date_column].dt.month == target_month_num)
                ].copy()
                
                if df_filtered.empty:
                    logging.warning(f"No data found for target month {target_month} in {country_name}")
                    df_filtered = df.copy()
                else:
                    logging.info(f"Monthly mode: filtered to {len(df_filtered)} rows for month {target_month}")
            else:
                df_filtered = df.copy()
        
        # Convert jobs column to numeric, handling any formatting issues
        df_filtered[jobs_column] = pd.to_numeric(
            df_filtered[jobs_column].astype(str).str.replace(',', '').str.strip(), 
            errors='coerce'
        )
        
        # Remove rows with invalid job counts
        df_filtered = df_filtered[pd.notna(df_filtered[jobs_column])].copy()
        
        if df_filtered.empty:
            logging.error(f"No valid job counts found in {country_name}")
            return False
        
        # Group by date only (not datetime) and sum jobs
        daily_totals = df_filtered.groupby('DATE_ONLY')[jobs_column].sum().reset_index()
        daily_totals.columns = ['DATE_ONLY', 'TOTAL_JOBS']
        
        # Format date as datetime string for output (matching database format)
        daily_totals['NET_DATE'] = pd.to_datetime(daily_totals['DATE_ONLY']).dt.strftime('%Y-%m-%d %H:%M:%S')
        
        # Select only the columns we need
        daily_totals = daily_totals[['NET_DATE', 'TOTAL_JOBS']]
        
        # Sort by date
        daily_totals = daily_totals.sort_values('NET_DATE')
        
        # Log summary of what we're writing
        logging.info(f"Processed {country_name}: {len(daily_totals)} daily records")
        logging.info(f"Date range in output: {daily_totals['NET_DATE'].min()} to {daily_totals['NET_DATE'].max()}")
        logging.info(f"Total jobs: {daily_totals['TOTAL_JOBS'].sum():,}")
        
        # Log first few rows of output
        logging.info("First 5 rows of output:")
        for i in range(min(5, len(daily_totals))):
            row = daily_totals.iloc[i]
            logging.info(f"  {row['NET_DATE']}: {row['TOTAL_JOBS']:,.0f} jobs")
        
        # Save to output file
        daily_totals.to_csv(output_file, index=False)
        
        logging.info(f"Successfully wrote {len(daily_totals)} daily aggregated records to {output_file}")
        
        return True
        
    except Exception as e:
        logging.error(f"Error processing CSV file for {country_name}: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return False


# ---------------------------------------------------------------------------
# Enhanced Multi-Country Workflow Function
# ---------------------------------------------------------------------------

def run_workflow_multi_enhanced(countries, default_output_csv, execution_timestamp, execution_user, 
                               test_mode=False, publish_test=True, target_month=None, is_daily_mode=False):
    """
    Enhanced multi-country workflow that handles both database countries and CSV file countries.
    FIXED: Pass daily mode flag to CSV processing
    
    Args:
        countries: List of database countries from countries.json
        target_month: String in format 'YYYYMM' (e.g., '202509' for September 2025)
        is_daily_mode: If True, process as YTD mode
    """
    # If no target month specified, use current month
    if not target_month:
        target_month = datetime.now().strftime('%Y%m')
    
    logging.info(f"="*60)
    logging.info(f"Starting ENHANCED multi-country workflow for month: {target_month}")
    logging.info(f"Mode: {'YTD/Daily' if is_daily_mode else 'Monthly'}")
    logging.info(f"Database countries: {[c['name'] for c in countries]}")
    logging.info(f"="*60)
    
    # Create date-based folder for organization
    today_folder = datetime.now().strftime('%Y-%m-%d')
    output_dir = os.path.join('reports', today_folder)
    os.makedirs(output_dir, exist_ok=True)
    
    original_cwd = os.getcwd()
    
    try:
        # Step 1: Process database countries (existing logic)
        country_results = []
        for country in countries:
            country_name = country['name']
            config_file = country['config_file']
            query_file = country['query_file']
            output_csv = os.path.join(output_dir, f"data_{country_name.lower()}.csv")
            
            logging.info(f"Processing database country: {country_name}")
            
            if test_mode:
                # Create dummy test data for database countries
                with open(output_csv, 'w') as f:
                    f.write("NET_DATE,TOTAL_JOBS\n")
                    for day in range(1, 31):
                        date_str = f"2025-09-{day:02d} 00:00:00"
                        jobs = 5000 + (day * 100)
                        f.write(f"{date_str},{jobs}\n")
                logging.info(f"Created test data for {country_name}")
            else:
                result = run_workflow(
                    config_file, query_file, output_csv,
                    execution_timestamp, execution_user,
                    test_mode=test_mode, publish_test=False
                )
                
                if result != 0:
                    logging.error(f"Failed to process database country {country_name}")
                    continue
            
            if os.path.exists(output_csv):
                country_results.append({
                    'name': country_name,
                    'csv_file': output_csv,
                    'success': True,
                    'source': 'database'
                })
                logging.info(f"Successfully processed database country: {country_name}")
        
        # Step 2: Process CSV file countries with daily mode flag
        csv_countries_processed = process_csv_file_countries(
            output_dir, target_month, test_mode, is_daily_mode=is_daily_mode
        )
        
        # Add CSV countries to results
        country_results.extend(csv_countries_processed)
        
        # Step 3: Verify we have data to process
        successful_countries = [r for r in country_results if r['success']]
        if not successful_countries:
            logging.error("No countries processed successfully")
            return False
        
        # Log summary of data sources
        db_countries = [r for r in successful_countries if r['source'] == 'database']
        csv_countries = [r for r in successful_countries if r['source'] == 'csv_file']
        
        logging.info(f"Data sources summary:")
        logging.info(f"  Database countries: {len(db_countries)} - {[r['name'] for r in db_countries]}")
        logging.info(f"  CSV file countries: {len(csv_countries)} - {[r['name'] for r in csv_countries]}")
        
        # Step 4: Change to output directory and aggregate all data
        os.chdir(output_dir)
        
        csv_files = [f for f in os.listdir('.') if f.startswith('data_') and f.endswith('.csv')]
        logging.info(f"CSV files found for aggregation: {csv_files}")
        
        # Use CSV processor to aggregate all country data
        processor = CSVProcessor(execution_timestamp, execution_user)
        success = processor.process_all_files()
        
        if not success:
            logging.error("Failed to aggregate country data")
            return False
        
        # Step 5: Copy aggregated reports back and publish
        aggregated_report = "task_usage_report_by_region.csv"
        main_aggregated_path = os.path.join(original_cwd, aggregated_report)
        
        if os.path.exists(aggregated_report):
            shutil.copy2(aggregated_report, main_aggregated_path)
            logging.info(f"Copied aggregated report to {main_aggregated_path}")
        
        os.chdir(original_cwd)
        
        # Step 6: Publish to Confluence
        if publish_test:
            logging.info("Publishing aggregated report to Confluence...")
            publish_success = publish_to_confluence(
                report_file=aggregated_report,
                test_mode=test_mode,
                skip_actual_upload=False
            )
            if not publish_success:
                logging.error("Failed to publish to Confluence")
                return False
        
        # Final summary
        logging.info("="*60)
        logging.info("ENHANCED MULTI-COUNTRY WORKFLOW SUMMARY")
        logging.info("="*60)
        logging.info(f"Target Month: {target_month}")
        logging.info(f"Mode: {'YTD/Daily' if is_daily_mode else 'Monthly'}")
        logging.info(f"Total Countries Processed: {len(successful_countries)}")
        logging.info(f"Database Countries: {len(db_countries)}")
        logging.info(f"CSV File Countries: {len(csv_countries)}")
        
        for result in successful_countries:
            status = "âœ“ SUCCESS"
            source_info = f"({result['source']})"
            logging.info(f"  {result['name']}: {status} {source_info}")
        
        return True
        
    except Exception as e:
        logging.error(f"Error in enhanced multi-country workflow: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return False
    finally:
        os.chdir(original_cwd)


# ---------------------------------------------------------------------------
# Main - FIXED to pass daily mode flag
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(description="Enhanced data workflow with CSV auto-discovery")
    parser.add_argument("--test", action="store_true", help="Run in test mode using predefined test CSV")
    parser.add_argument("--no-publish", action="store_true", help="Skip actual publishing to Confluence")
    parser.add_argument("--monthly", action="store_true", help="Use previous month date range in SQL query")
    parser.add_argument("--daily", action="store_true", help="Use year-to-date range in SQL query")
    parser.add_argument("--countries-config", help="Path to countries JSON to run multi-country workflow")
    parser.add_argument("--target-month", help="Target month in YYYYMM format (e.g., 202509)")
    parser.add_argument("--csv-countries-only", action="store_true", help="Process only CSV file countries")
    parser.add_argument("--list-csv-countries", action="store_true", help="List available CSV countries")
    args = parser.parse_args()

    setup_logging()
    os.makedirs('config', exist_ok=True)

    # Determine target month with proper monthly logic
    if args.target_month:
        target_month = args.target_month
    elif args.monthly:
        # For monthly mode, use previous completed month
        today = datetime.now()
        first_day_current = datetime(today.year, today.month, 1)
        last_day_prev = first_day_current - timedelta(days=1)
        target_month = last_day_prev.strftime('%Y%m')
        logging.info(f"Monthly mode: targeting previous completed month {target_month}")
    else:
        # Default to current month for daily or no-flag runs
        target_month = datetime.now().strftime('%Y%m')
        logging.info(f"Default mode: targeting current month {target_month}")

    # [Keep all the CSV-only operations the same]

    if args.monthly and args.daily:
        logging.error("Cannot use both --monthly and --daily flags together.")
        return 1

    date_range = None
    month_name = None
    if args.monthly:
        sd, ed, month_name = get_previous_month_date_range()
        date_range = (sd, ed, month_name)
    elif args.daily:
        sd, ed = get_ytd_date_range()
        date_range = (sd, ed, None)

    # [Keep authentication check the same]

    # -----------------------------------------------------------------------
    # Multi-country Flow (Enhanced) - FIXED
    # -----------------------------------------------------------------------
    if args.countries_config:
        try:
            with open(args.countries_config, "r", encoding='utf-8') as f:
                countries_cfg = json.load(f)
            countries = countries_cfg.get("countries", [])
            if not countries:
                logging.error(f"No countries defined in {args.countries_config}")
                return 1
        except Exception as e:
            logging.error(f"Failed to read countries config: {e}")
            return 1

        prepared = []
        for c in countries:
            name = c.get("name")
            cfg_path = c.get("config_file") or CONFIG_FILE
            qry_path = c.get("query_file") or QUERY_FILE
            if not name or not cfg_path:
                logging.error("Each country must have 'name' and 'config_file'")
                return 1
            if not args.test and not os.path.exists(cfg_path):
                logging.error(f"Config file for {name} not found: {cfg_path}")
                return 1
            if not args.test and not os.path.exists(qry_path):
                logging.error(f"Query file for {name} not found: {qry_path}")
                return 1

            # Handle date range updates if specified
            if date_range:
                sd, ed, _ = date_range
                tmp_qry = os.path.join(tempfile.gettempdir(), f"query_{name}.sql")
                with open(qry_path, 'r', encoding='utf-8') as src, open(tmp_qry, 'w', encoding='utf-8') as dst:
                    dst.write(src.read())
                if not update_query_dates_preserve(tmp_qry, sd, ed):
                    logging.error(f"Failed to update date range for {name}")
                    return 1
                prepared.append({"name": name, "config_file": cfg_path, "query_file": tmp_qry})
            else:
                prepared.append({"name": name, "config_file": cfg_path, "query_file": qry_path})

        # Update title ONCE (global JSON) not per country
        if args.monthly and month_name:
            update_confluence_page_title(TITLE_CONFIG_FILE, month_name, is_daily=False)
        elif args.daily:
            update_confluence_page_title(TITLE_CONFIG_FILE, "YTD", is_daily=True)

        # Determine if we're in daily mode
        is_daily_mode = args.daily
        
        # Use enhanced workflow with daily mode flag
        result = run_workflow_multi_enhanced(
            countries=prepared,
            default_output_csv=OUTPUT_CSV,
            execution_timestamp=EXECUTION_TIMESTAMP,
            execution_user=EXECUTION_USER,
            test_mode=args.test,
            publish_test=not args.no_publish,
            target_month=target_month,
            is_daily_mode=is_daily_mode  # Pass the flag here!
        )
        return 0 if result else 1

    # [Keep single-country flow the same]

    return result


# ---------------------------------------------------------------------------
# Entrypoint
# ---------------------------------------------------------------------------

if __name__ == "__main__":
    sys.exit(main())
