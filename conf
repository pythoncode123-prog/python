import requests
from requests.auth import HTTPBasicAuth
import pandas as pd
import json
import urllib3
import os
from datetime import datetime
import logging
import re
from typing import Tuple, Optional, Dict, List

# -------------------------------------------------------------
# Confluence Publisher - FIXED: No legacy detection when mode is explicit
# -------------------------------------------------------------

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

MONTH_NAMES = (
    "January", "February", "March", "April", "May", "June",
    "July", "August", "September", "October", "November", "December"
)

# =============================================================
# Configuration / Auth
# =============================================================

def find_config_file(test_mode: bool = False) -> Optional[str]:
    """
    Find the config file in various possible locations.
    Returns the path to the first existing config file found.
    """
    # Define search paths based on mode
    if test_mode:
        search_paths = [
            "config/config_test.json",
            "config_test.json",
            "config/config.json",
            "config.json"
        ]
    else:
        search_paths = [
            "config/config.json",
            "config.json",
            "config/config_test.json",
            "config_test.json"
        ]
    
    # Check each path
    for path in search_paths:
        abs_path = os.path.abspath(path)
        if os.path.exists(abs_path):
            logging.info(f"Found config file at: {abs_path}")
            return abs_path
    
    # Log all attempted paths if none found
    logging.error(f"Config file not found. Searched in: {search_paths}")
    return None

def load_config(config_path: str = None) -> Optional[Dict]:
    """
    Load configuration from the specified or auto-detected path.
    """
    try:
        # If no path specified, try to find it
        if not config_path:
            config_path = find_config_file()
            if not config_path:
                logging.error("Could not find config file in any expected location")
                return None
        
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        required = ['CONFLUENCE_URL', 'USERNAME', 'SPACE_KEY', 'PAGE_TITLE']
        if not all(k in config for k in required):
            logging.error(f"Missing required configuration keys. Need: {required}")
            return None
        
        if 'BASELINE' not in config:
            config['BASELINE'] = 1899206
        
        logging.info(f"Successfully loaded config from {config_path}")
        return config
    except Exception as e:
        logging.error(f"Error loading config from {config_path}: {e}")
        return None

def get_password_from_config(config: Dict) -> Optional[str]:
    """
    Get password from config - either encrypted or base64 encoded.
    """
    # Try encrypted password first
    if 'PASSWORD_ENCRYPTED' in config:
        try:
            from lib.secure_config import SecureConfig
            password = SecureConfig.decrypt_password(config['PASSWORD_ENCRYPTED'])
            if password:
                return password
        except Exception as e:
            logging.warning(f"Could not decrypt password: {e}")
    
    # Try base64 encoded password
    if 'PASSWORD_B64' in config:
        try:
            import base64
            password = base64.b64decode(config['PASSWORD_B64']).decode('utf-8')
            if password:
                return password
        except Exception as e:
            logging.warning(f"Could not decode base64 password: {e}")
    
    # Try plain password (not recommended)
    if 'PASSWORD' in config:
        return config['PASSWORD']
    
    return None

def create_session(config: Dict) -> Optional[requests.Session]:
    """
    Create authenticated session for Confluence API.
    """
    session = requests.Session()
    auth_type = config.get('AUTH_TYPE', 'basic').lower()
    
    if auth_type == 'basic':
        # Try various password sources
        password = (os.environ.get('CONFLUENCE_PASSWORD') or
                    get_password_from_config(config) or
                    config.get('API_TOKEN'))
        
        if not password:
            logging.error("Missing credentials for basic auth.")
            return None
        
        session.auth = HTTPBasicAuth(config['USERNAME'], password)
        
    elif auth_type == 'jwt':
        token = os.environ.get('CONFLUENCE_TOKEN') or config.get('API_TOKEN')
        if not token:
            logging.error("Missing JWT token.")
            return None
        session.headers.update({"Authorization": f"Bearer {token}"})
        
    elif auth_type == 'cookie':
        cookie = os.environ.get('CONFLUENCE_COOKIE') or config.get('SESSION_COOKIE')
        if not cookie:
            logging.error("Missing cookie value for cookie auth.")
            return None
        session.headers.update({"Cookie": cookie})
    else:
        logging.error(f"Unsupported AUTH_TYPE {auth_type}")
        return None

    session.headers.update({
        "Content-Type": "application/json",
        "X-Atlassian-Token": "no-check"
    })
    
    if 'PROXY' in config:
        session.proxies = {"http": config['PROXY'], "https": config['PROXY']}
    
    session.verify = False
    return session

# =============================================================
# Data Loading & Sanitization
# =============================================================

def load_csv_data(csv_file: str) -> Tuple[pd.DataFrame, bool]:
    try:
        df = pd.read_csv(csv_file)
        if 'DATE' in df.columns:
            df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')
        return df, True
    except Exception as e:
        logging.error(f"Error loading CSV {csv_file}: {e}")
        return pd.DataFrame(), False

def sanitize_total_jobs(df: pd.DataFrame,
                        preferred_col: Optional[str] = None) -> pd.DataFrame:
    """
    Ensure we have a numeric TOTAL_JOBS column.
    """
    candidate = None
    if preferred_col and preferred_col in df.columns:
        candidate = preferred_col
    elif 'TOTAL_JOBS' in df.columns:
        candidate = 'TOTAL_JOBS'
    else:
        # Try to guess a column with large ints
        for c in df.columns:
            if re.search(r'jobs|count|total', c.lower()):
                candidate = c
                break

    if candidate is None:
        logging.error("No suitable column found for TOTAL_JOBS.")
        df['TOTAL_JOBS'] = pd.Series(dtype='float')
        return df

    # Create/overwrite TOTAL_JOBS from candidate
    cleaned = (
        df[candidate]
        .astype(str)
        .str.replace(',', '', regex=False)
        .str.replace(' ', '', regex=False)
        .str.strip()
    )
    df['TOTAL_JOBS'] = pd.to_numeric(cleaned, errors='coerce')

    return df

# =============================================================
# Peak Calculation - FIXED FOR YTD IN DAILY MODE
# =============================================================

def latest_year_month(daily_df: pd.DataFrame) -> Tuple[int, int]:
    latest = daily_df['DATE'].max()
    return latest.year, latest.month

def filter_to_month(daily_df: pd.DataFrame,
                    year: Optional[int],
                    month: Optional[int]) -> pd.DataFrame:
    if year is None or month is None:
        year, month = latest_year_month(daily_df)
    result = daily_df[(daily_df.DATE.dt.year == year) & (daily_df.DATE.dt.month == month)].copy()
    return result, year, month

def compute_top_peaks(daily_df: pd.DataFrame,
                      top_n: int,
                      year: Optional[int],
                      month: Optional[int]) -> Tuple[pd.DataFrame, int, int]:
    """Compute top peaks for a specific month - excluding today if current month"""
    today = datetime.now().date()
    
    # Filter out today's data
    daily_df = daily_df[daily_df['DATE'].dt.date < today].copy()
    
    month_df, y, m = filter_to_month(daily_df, year, month)
    if month_df.empty:
        return month_df, y, m
    try:
        peaks = month_df.nlargest(top_n, 'TOTAL_JOBS')
    except Exception:
        peaks = month_df.sort_values('TOTAL_JOBS', ascending=False).head(top_n)
    return peaks[['DATE', 'TOTAL_JOBS']], y, m

def compute_top_peaks_ytd(daily_df: pd.DataFrame, top_n: int = 4) -> pd.DataFrame:
    """
    FIXED: Compute top N peaks for Year-To-Date (for daily reports) - excluding today
    This function should NOT filter to any specific month, only to YTD
    """
    if daily_df.empty:
        return daily_df
    
    # Get current date and year
    today = datetime.now().date()
    current_year = today.year
    
    # IMPORTANT: Do NOT filter to any specific month, only filter to current year
    # and exclude today's data
    ytd_df = daily_df[
        (daily_df['DATE'].dt.year == current_year) & 
        (daily_df['DATE'].dt.date < today)  # Exclude today
    ].copy()
    
    if ytd_df.empty:
        # Fallback to all data except today if no current year data
        ytd_df = daily_df[daily_df['DATE'].dt.date < today].copy()
    
    # Log the actual date range being used
    if not ytd_df.empty:
        min_date = ytd_df['DATE'].min()
        max_date = ytd_df['DATE'].max()
        total_days = len(ytd_df['DATE'].unique())
        logging.info(f"YTD Peak Calculation: Using data from {min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')} ({total_days} unique days)")
        logging.info(f"YTD data contains {len(ytd_df)} total records from year {current_year}")
    
    # Get top N peaks by total jobs from ENTIRE YTD period
    try:
        peaks = ytd_df.nlargest(top_n, 'TOTAL_JOBS')
    except Exception:
        peaks = ytd_df.sort_values('TOTAL_JOBS', ascending=False).head(top_n)
    
    result = peaks[['DATE', 'TOTAL_JOBS']].sort_values('DATE')
    
    # Log the peaks found
    if not result.empty:
        logging.info(f"Top {top_n} YTD peaks found:")
        for _, row in result.iterrows():
            logging.info(f"  {row['DATE'].strftime('%Y-%m-%d')}: {int(row['TOTAL_JOBS']):,} jobs")
    
    return result

# =============================================================
# Chart Builders
# =============================================================

def peaks_transposed_chart(peaks_df: pd.DataFrame,
                           baseline: int,
                           chart_title: str) -> str:
    """
    Create chart with transposed data (dates as columns, series as rows)
    Using specific color codes: red #B83939 for baseline, green #829A3D for total jobs
    """
    if peaks_df.empty:
        return f"<p>No data for {chart_title}.</p>"
    
    # Sort by date
    peaks_df = peaks_df.sort_values('DATE')
    
    # Format dates as MM-DD for cleaner display
    date_format = peaks_df['DATE'].dt.strftime('%m-%d')
    
    # First row: Date headers
    header_row = "<tr><th>Date</th>"
    for date in date_format:
        header_row += f"<th>{date}</th>"
    header_row += "</tr>"
    
    # Second row: Baseline values
    baseline_row = "<tr><td>Baseline</td>"
    for _ in range(len(peaks_df)):
        baseline_row += f"<td>{int(baseline)}</td>"
    baseline_row += "</tr>"
    
    # Third row: Total Jobs values
    jobs_row = "<tr><td>Total Jobs</td>"
    for _, row in peaks_df.iterrows():
        jobs_row += f"<td>{int(row['TOTAL_JOBS'])}</td>"
    jobs_row += "</tr>"
    
    return f"""
<ac:structured-macro ac:name="chart">
  <ac:parameter ac:name="title">{chart_title}</ac:parameter>
  <ac:parameter ac:name="type">bar</ac:parameter>
  <ac:parameter ac:name="orientation">vertical</ac:parameter>
  <ac:parameter ac:name="3D">true</ac:parameter>
  <ac:parameter ac:name="width">600</ac:parameter>
  <ac:parameter ac:name="height">400</ac:parameter>
  <ac:parameter ac:name="legend">true</ac:parameter>
  <ac:parameter ac:name="dataDisplay">true</ac:parameter>
  <ac:parameter ac:name="stacked">false</ac:parameter>
  <ac:parameter ac:name="showValues">true</ac:parameter>
  <ac:parameter ac:name="colors">#B83939,#829A3D</ac:parameter>
  <ac:rich-text-body>
    <table>
      <tbody>
        {header_row}
        {baseline_row}
        {jobs_row}
      </tbody>
    </table>
  </ac:rich-text-body>
</ac:structured-macro>
"""

def generate_jobs_data_table(df: pd.DataFrame, baseline: int, is_daily: bool = False) -> str:
    """
    Generate the Jobs Data table showing ONLY TOP 4 PEAKS with Date, Baseline, Total Jobs, and Variation.
    For daily mode, shows YTD peaks. Excludes today's data.
    """
    # Filter out today's data first
    today = datetime.now().date()
    df_filtered = df[df['DATE'].dt.date < today].copy()
    
    # Group by date and sum total jobs
    daily = df_filtered.groupby('DATE', as_index=False)['TOTAL_JOBS'].sum()
    
    if daily.empty:
        return "<p>No daily data available.</p>"
    
    # Get top 4 peaks based on mode
    if is_daily:
        # For daily mode, use YTD data for peaks
        logging.info("Generating Jobs Data table for DAILY mode - using YTD peaks")
        top_4_peaks = compute_top_peaks_ytd(daily, 4)
        title = "Jobs Data - Top 4 Peaks (YTD)"
    else:
        # For monthly mode, use current month
        logging.info("Generating Jobs Data table for MONTHLY mode - using current month peaks")
        top_4_peaks, _, _ = compute_top_peaks(daily, 4, None, None)
        title = "Jobs Data - Top 4 Peaks"
    
    if top_4_peaks.empty:
        return f"<p>No peak data available for {title}</p>"
    
    # Start the table
    html = f"""
<h3>{title}</h3>
<table class='wrapped'>
<tbody>
<tr>
    <th>Date</th>
    <th>Baseline</th>
    <th>Total Jobs</th>
    <th>Variation</th>
</tr>
"""
    
    # Add data rows for top 4 only
    for _, row in top_4_peaks.iterrows():
        date_str = row['DATE'].strftime('%m/%d/%Y')
        total_jobs = int(row['TOTAL_JOBS'])
        variation = baseline - total_jobs
        
        # Color coding logic
        if variation > 0:
            var_style = 'style="background-color:#90EE90;"'
            var_text = f"{variation:,}"
        elif variation < 0:
            var_style = 'style="background-color:#FFB6C1;"'
            var_text = f"{variation:,}"
        else:
            var_style = ''
            var_text = "0"
        
        html += f"""
<tr>
    <td>{date_str}</td>
    <td>{baseline:,}</td>
    <td>{total_jobs:,}</td>
    <td {var_style}>{var_text}</td>
</tr>
"""
    
    html += "</tbody></table>"
    return html

def generate_global_peaks_section(df: pd.DataFrame,
                                  baseline: int,
                                  top_n: int = 4,
                                  is_daily: bool = False) -> str:
    """
    Generate the peak comparison chart.
    For daily mode: shows YTD peaks
    For monthly mode: shows current month peaks
    """
    # Filter out today's data first
    today = datetime.now().date()
    df_filtered = df[df['DATE'].dt.date < today].copy()
    
    daily = df_filtered.groupby('DATE', as_index=False)['TOTAL_JOBS'].sum()
    
    if daily.empty:
        return "<p>No data available for peaks chart.</p>"
    
    if is_daily:
        logging.info("Generating peaks chart for DAILY mode - using YTD data")
        peaks = compute_top_peaks_ytd(daily, top_n)
        chart_title = "Top 4 Peaks of YTD"
    else:
        logging.info("Generating peaks chart for MONTHLY mode - using current month")
        peaks, y, m = compute_top_peaks(daily, top_n, None, None)
        if peaks.empty:
            return "<p>No peak data for current month.</p>"
        # Get the month from the actual data
        month_label = peaks['DATE'].dt.strftime('%b').iloc[0] if not peaks.empty else "Current Month"
        chart_title = f"Top {len(peaks)} Peaks of {month_label}"
    
    if peaks.empty:
        return "<p>No peak data available.</p>"
        
    chart = peaks_transposed_chart(peaks, baseline, chart_title)
    return chart

def generate_monthly_usage_chart(df: pd.DataFrame, baseline: int) -> str:
    """
    Generate Overall Monthly Task Usage Report line chart - excluding today's data
    """
    # Filter out today's data
    today = datetime.now().date()
    df_filtered = df[df['DATE'].dt.date < today].copy()
    
    # Group by date and sum
    daily = df_filtered.groupby('DATE', as_index=False)['TOTAL_JOBS'].sum().sort_values('DATE')
    
    if daily.empty:
        return "<p>No data for monthly usage chart.</p>"
    
    # Build the data table for the chart
    header_row = "<tr><th>Date</th>"
    for _, row in daily.iterrows():
        date_str = row['DATE'].strftime('%Y-%m-%d')
        header_row += f"<th>{date_str}</th>"
    header_row += "</tr>"
    
    # Baseline row
    baseline_row = "<tr><td>Baseline</td>"
    for _ in range(len(daily)):
        baseline_row += f"<td>{baseline}</td>"
    baseline_row += "</tr>"
    
    # Total Jobs row
    jobs_row = "<tr><td>Sum of TOTAL_JOBS</td>"
    for _, row in daily.iterrows():
        jobs_row += f"<td>{int(row['TOTAL_JOBS'])}</td>"
    jobs_row += "</tr>"
    
    return f"""
<h3>Overall Monthly Task Usage Report</h3>
<ac:structured-macro ac:name="chart">
  <ac:parameter ac:name="title">Overall Monthly Task Usage Report</ac:parameter>
  <ac:parameter ac:name="type">line</ac:parameter>
  <ac:parameter ac:name="width">1200</ac:parameter>
  <ac:parameter ac:name="height">400</ac:parameter>
  <ac:parameter ac:name="3D">false</ac:parameter>
  <ac:parameter ac:name="legend">true</ac:parameter>
  <ac:parameter ac:name="showValues">false</ac:parameter>
  <ac:parameter ac:name="colors">#829A3D,#B83939</ac:parameter>
  <ac:parameter ac:name="dataDisplay">after</ac:parameter>
  <ac:parameter ac:name="xLabel">Date</ac:parameter>
  <ac:parameter ac:name="yLabel">Jobs</ac:parameter>
  <ac:rich-text-body>
    <table>
      <tbody>
        {header_row}
        {baseline_row}
        {jobs_row}
      </tbody>
    </table>
  </ac:rich-text-body>
</ac:structured-macro>
"""

def generate_monthly_country_breakdown_chart(df: pd.DataFrame) -> str:
    """
    Generate monthly stacked bar chart broken down by country.
    Excludes today's data.
    """
    # Filter out today's data
    today = datetime.now().date()
    df = df[df['DATE'].dt.date < today].copy()
    
    # Check if we have country data
    country_col = None
    possible_country_cols = ['COUNTRY', 'Country', 'country', 'REGION', 'Region', 'region']
    
    for col in possible_country_cols:
        if col in df.columns:
            country_col = col
            break
    
    if country_col is None:
        return ""
    
    # Add month columns
    df_copy = df.copy()
    df_copy['Month'] = df_copy['DATE'].dt.strftime('%b')
    df_copy['MonthNum'] = df_copy['DATE'].dt.month
    df_copy['Year'] = df_copy['DATE'].dt.year
    
    # Create pivot table for monthly breakdown
    monthly_pivot = df_copy.pivot_table(
        values='TOTAL_JOBS',
        index=['Year', 'MonthNum', 'Month'],
        columns=country_col,
        aggfunc='sum',
        fill_value=0
    ).reset_index()
    
    # Sort by year and month
    monthly_pivot = monthly_pivot.sort_values(['Year', 'MonthNum'])
    
    # Get list of countries
    countries = sorted([col for col in monthly_pivot.columns if col not in ['Year', 'MonthNum', 'Month']])
    
    # Build chart data
    header_row = "<tr><th>Country</th>"
    for _, row in monthly_pivot.iterrows():
        header_row += f"<th>{row['Month']}</th>"
    header_row += "</tr>"
    
    # Add data rows for each country
    data_rows = []
    for country in countries:
        country_row = f"<tr><td>Sum of {country}</td>"
        for _, row in monthly_pivot.iterrows():
            value = int(row[country]) if country in row and pd.notna(row[country]) else 0
            country_row += f"<td>{value}</td>"
        country_row += "</tr>"
        data_rows.append(country_row)
    
    table_html = f"""<table><tbody>{header_row}{''.join(data_rows)}</tbody></table>"""
    
    # Also create summary table
    summary_html = """
<h3>Monthly Summary by Country</h3>
<table class='wrapped'>
<tbody>
<tr>
    <th>Months</th>
"""
    
    for country in countries:
        summary_html += f"<th>Sum of {country}</th>"
    summary_html += "</tr>"
    
    for _, row in monthly_pivot.iterrows():
        summary_html += f"<tr><td>{row['Month']}</td>"
        for country in countries:
            value = int(row[country]) if country in row and pd.notna(row[country]) else 0
            summary_html += f"<td>{value:,}</td>"
        summary_html += "</tr>"
    
    summary_html += "</tbody></table>"
    
    # Generate stacked bar chart
    chart_html = f"""
<h3>Monthly Jobs by Country</h3>
<ac:structured-macro ac:name="chart">
  <ac:parameter ac:name="type">bar</ac:parameter>
  <ac:parameter ac:name="title">Monthly Jobs by Country</ac:parameter>
  <ac:parameter ac:name="width">1400</ac:parameter>
  <ac:parameter ac:name="height">700</ac:parameter>
  <ac:parameter ac:name="3D">true</ac:parameter>
  <ac:parameter ac:name="legend">true</ac:parameter>
  <ac:parameter ac:name="orientation">vertical</ac:parameter>
  <ac:parameter ac:name="stacked">true</ac:parameter>
  <ac:parameter ac:name="showValues">true</ac:parameter>
  <ac:parameter ac:name="xaxisLabel">Month</ac:parameter>
  <ac:parameter ac:name="yaxisLabel">Jobs</ac:parameter>
  <ac:parameter ac:name="dataDisplay">after</ac:parameter>
  <ac:rich-text-body>
    {table_html}
  </ac:rich-text-body>
</ac:structured-macro>
"""
    
    return summary_html + chart_html

def generate_daily_total_single_series(df: pd.DataFrame,
                                       baseline: int,
                                       title: str = "Daily Total Jobs") -> str:
    """
    Generate daily totals chart - simple for monthly mode
    Excludes today's data.
    """
    # Filter out today's data
    today = datetime.now().date()
    df_filtered = df[df['DATE'].dt.date < today].copy()
    
    daily = df_filtered.groupby('DATE', as_index=False)['TOTAL_JOBS'].sum().sort_values('DATE')
    if daily.empty:
        return "<p>No daily data.</p>"
    
    # Create header row
    header_row = "<tr><th>Series</th>"
    for _, row in daily.iterrows():
        date_str = row['DATE'].strftime('%d-%m')
        header_row += f"<th>{date_str}</th>"
    header_row += "</tr>"
    
    # Create data row
    data_row = "<tr><td>Total Jobs</td>"
    for _, row in daily.iterrows():
        data_row += f"<td>{int(row['TOTAL_JOBS'])}</td>"
    data_row += "</tr>"
    
    table_html = f"""<table><tbody>{header_row}{data_row}</tbody></table>"""
    
    chart = f"""
<h3>{title}</h3>
<ac:structured-macro ac:name="chart">
  <ac:parameter ac:name="type">bar</ac:parameter>
  <ac:parameter ac:name="title">{title}</ac:parameter>
  <ac:parameter ac:name="width">1200</ac:parameter>
  <ac:parameter ac:name="height">600</ac:parameter>
  <ac:parameter ac:name="3D">true</ac:parameter>
  <ac:parameter ac:name="legend">false</ac:parameter>
  <ac:parameter ac:name="orientation">vertical</ac:parameter>
  <ac:parameter ac:name="labelAngle">90</ac:parameter>
  <ac:parameter ac:name="stacked">false</ac:parameter>
  <ac:parameter ac:name="showValues">true</ac:parameter>
  <ac:parameter ac:name="xaxisLabel">Date</ac:parameter>
  <ac:parameter ac:name="yaxisLabel">Jobs</ac:parameter>
  <ac:parameter ac:name="colors">#829A3D</ac:parameter>
  <ac:parameter ac:name="dataDisplay">after</ac:parameter>
  <ac:parameter ac:name="fontSize">10</ac:parameter>
  <ac:rich-text-body>
    {table_html}
  </ac:rich-text-body>
</ac:structured-macro>
"""
    return chart

def generate_country_summary_chart(df: pd.DataFrame,
                                  title: str = "Total Jobs by Country") -> str:
    """
    Generate bar chart showing total jobs per country with summary table.
    Excludes today's data.
    """
    # Filter out today's data
    today = datetime.now().date()
    df = df[df['DATE'].dt.date < today].copy()
    
    # Check if we have country data
    country_col = None
    possible_country_cols = ['COUNTRY', 'Country', 'country', 'REGION', 'Region', 'region']
    
    for col in possible_country_cols:
        if col in df.columns:
            country_col = col
            break
    
    if country_col is None:
        return "<p>No country data available for country summary chart.</p>"
    
    # Group by country and sum total jobs
    country_totals = df.groupby(country_col, as_index=False)['TOTAL_JOBS'].sum()
    country_totals = country_totals.sort_values('TOTAL_JOBS', ascending=False)
    
    if country_totals.empty:
        return "<p>No country data available.</p>"
    
    # Create summary table
    table_html = f"""
<h4>Country Summary</h4>
<table class='wrapped'>
<tbody>
<tr>
    <th>Country/Market</th>
    <th>Total Jobs</th>
    <th>Percentage</th>
</tr>
"""
    
    total_all_jobs = country_totals['TOTAL_JOBS'].sum()
    
    for _, row in country_totals.iterrows():
        country = row[country_col]
        jobs = int(row['TOTAL_JOBS'])
        percentage = (jobs / total_all_jobs * 100) if total_all_jobs > 0 else 0
        table_html += f"""
<tr>
    <td>{country}</td>
    <td>{jobs:,}</td>
    <td>{percentage:.1f}%</td>
</tr>
"""
    
    table_html += "</tbody></table>"
    
    # Create chart table data
    chart_header = "<tr><th>Country/Market</th><th>Task Count</th></tr>"
    chart_rows = []
    for _, row in country_totals.iterrows():
        country = row[country_col]
        jobs = int(row['TOTAL_JOBS'])
        chart_rows.append(f"<tr><td>{country}</td><td>{jobs}</td></tr>")
    
    chart_table_html = f"""<table><tbody>{chart_header}{''.join(chart_rows)}</tbody></table>"""
    
    chart = f"""
<h3>{title}</h3>
<ac:structured-macro ac:name="chart">
  <ac:parameter ac:name="type">bar</ac:parameter>
  <ac:parameter ac:name="title">{title}</ac:parameter>
  <ac:parameter ac:name="width">1200</ac:parameter>
  <ac:parameter ac:name="height">500</ac:parameter>
  <ac:parameter ac:name="3D">true</ac:parameter>
  <ac:parameter ac:name="legend">false</ac:parameter>
  <ac:parameter ac:name="orientation">vertical</ac:parameter>
  <ac:parameter ac:name="stacked">false</ac:parameter>
  <ac:parameter ac:name="showValues">true</ac:parameter>
  <ac:parameter ac:name="xaxisLabel">Country/Market</ac:parameter>
  <ac:parameter ac:name="yaxisLabel">Task Count</ac:parameter>
  <ac:parameter ac:name="colors">#829A3D</ac:parameter>
  <ac:parameter ac:name="dataDisplay">after</ac:parameter>
  <ac:parameter ac:name="labelAngle">45</ac:parameter>
  <ac:rich-text-body>
    {chart_table_html}
  </ac:rich-text-body>
</ac:structured-macro>
"""
    
    return table_html + chart

# =============================================================
# Confluence REST Helpers
# =============================================================

def get_page_info(session: requests.Session, config: Dict) -> Tuple[Optional[str], Optional[int]]:
    try:
        base_url = config['CONFLUENCE_URL'].rstrip('/')
        if not base_url.endswith('/content'):
            if base_url.endswith('/rest/api'):
                base_url = base_url + "/content"
            else:
                base_url = base_url + "/rest/api/content"
        
        params = {
            'title': config['PAGE_TITLE'],
            'spaceKey': config['SPACE_KEY'],
            'expand': 'version'
        }
        resp = session.get(base_url, params=params)
        
        if resp.status_code != 200:
            alt = f"{base_url}/search?cql=space={config['SPACE_KEY']} AND title=\"{config['PAGE_TITLE']}\""
            resp = session.get(alt)
        
        if resp.status_code != 200:
            logging.error(f"Page lookup failed: {resp.status_code} - {resp.text}")
            return None, None
        
        data = resp.json()
        if 'results' in data and data.get('size', 0) > 0:
            return data['results'][0]['id'], data['results'][0]['version']['number']
        if isinstance(data, list) and data:
            return data[0]['id'], data[0]['version']['number']
        return None, None
    except Exception as e:
        logging.error(f"Error getting page info: {e}")
        return None, None

def create_or_update_page(session: requests.Session,
                          config: Dict,
                          content: str,
                          page_id: Optional[str] = None,
                          version: Optional[int] = None) -> bool:
    try:
        base_url = config['CONFLUENCE_URL'].rstrip('/')
        if not base_url.endswith('/content'):
            if base_url.endswith('/rest/api'):
                base_url = base_url + "/content"
            else:
                base_url = base_url + "/rest/api/content"
        
        payload = {
            "type": "page",
            "title": config['PAGE_TITLE'],
            "space": {"key": config['SPACE_KEY']},
            "body": {"storage": {"value": content, "representation": "storage"}}
        }
        
        if page_id and version is not None:
            payload["id"] = page_id
            payload["version"] = {"number": version + 1}
            resp = session.put(f"{base_url}/{page_id}", json=payload)
        else:
            resp = session.post(base_url, json=payload)
        
        if resp.status_code >= 400:
            logging.error(f"Create/Update failed: {resp.status_code} - {resp.text}")
            return False
        
        logging.info(f"Page '{config['PAGE_TITLE']}' {'updated' if page_id else 'created'} successfully.")
        return True
    except Exception as e:
        logging.error(f"Error creating/updating page: {e}")
        return False

def test_connection(session: requests.Session, config: Dict) -> bool:
    try:
        base = config['CONFLUENCE_URL'].rstrip('/')
        if base.endswith('/content'):
            base = base[:-8]
        elif not base.endswith('/rest/api'):
            base = base + '/rest/api'
        test_url = f"{base}/space"
        resp = session.get(test_url, params={'limit': 1})
        return resp.status_code == 200
    except Exception as e:
        logging.error(f"Connection test exception: {e}")
        return False

# =============================================================
# Main Publish Function - FIXED: NO LEGACY DETECTION
# =============================================================

def publish_to_confluence(report_file='task_usage_report_by_region.csv',
                          test_mode=False,
                          skip_actual_upload: bool = False,
                          is_daily_mode: bool = False) -> bool:
    """
    Publish report to Confluence with explicit mode control.
    FIXED: Always use explicit mode, never detect from title.
    
    Args:
        report_file: Path to the CSV report file
        test_mode: If True, append "-TEST" to page title
        skip_actual_upload: If True, skip actual upload
        is_daily_mode: If True, use YTD mode; if False, use monthly mode
    """
    try:
        # Check report file exists
        if not os.path.exists(report_file):
            logging.error(f"Report file {report_file} not found.")
            return False
        
        # Find and load config
        cfg_path = find_config_file(test_mode)
        if not cfg_path:
            logging.error("Could not find config file")
            return False
        
        config = load_config(cfg_path)
        if not config:
            logging.error("Failed to load config")
            return False
        
        # Add test suffix if in test mode
        if test_mode and not config['PAGE_TITLE'].endswith("-TEST"):
            config['PAGE_TITLE'] += "-TEST"

        # FIXED: Always use the explicit mode passed as parameter
        # NO LEGACY DETECTION FROM TITLE
        is_daily = is_daily_mode
        logging.info(f"Using explicit mode from parameter: {'DAILY (YTD)' if is_daily else 'MONTHLY'}")
        
        baseline = config.get('BASELINE', 1899206)

        logging.info("="*60)
        logging.info(f"Publishing to Confluence")
        logging.info(f"Mode: {'DAILY (YTD)' if is_daily else 'MONTHLY'}")
        logging.info(f"Page title: {config['PAGE_TITLE']}")
        logging.info(f"Baseline: {baseline:,}")
        logging.info(f"Today's date (excluded): {datetime.now().date()}")
        logging.info("="*60)

        # Load and sanitize data
        df, ok = load_csv_data(report_file)
        if not ok:
            logging.error("Failed to load CSV data")
            return False
        
        df = sanitize_total_jobs(df, preferred_col=config.get('JOB_COLUMN'))

        # Create session if not skipping upload
        session = None
        if not skip_actual_upload:
            session = create_session(config)
            if not session:
                logging.error("Failed to create session")
                return False
            
            if not test_connection(session, config):
                logging.error("Connection test failed.")
                return False

        # Generate content sections (all will exclude today's data and use proper YTD for daily)
        peaks_section = generate_global_peaks_section(df, baseline, is_daily=is_daily)
        jobs_data_table = generate_jobs_data_table(df, baseline, is_daily=is_daily)
        monthly_usage_chart = generate_monthly_usage_chart(df, baseline)
        
        # For daily mode, show monthly country breakdown chart and table
        if is_daily:
            logging.info("Generating daily mode charts with YTD peaks and country breakdown")
            monthly_country_chart = generate_monthly_country_breakdown_chart(df)
        else:
            logging.info("Generating monthly mode charts with current month peaks")
            monthly_country_chart = ""
            
        # Simple daily chart only for monthly mode
        if not is_daily:
            daily_chart = generate_daily_total_single_series(df, baseline, "Daily Total Jobs")
        else:
            daily_chart = ""
            
        country_chart = generate_country_summary_chart(df, "Total Jobs by Country")

        # Create page content
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        user = os.environ.get('USER', 'automated')
        yesterday = (datetime.now().date() - pd.Timedelta(days=1)).strftime('%Y-%m-%d')
        
        title_suffix = " - YTD" if is_daily else ""
        date_range_desc = "YTD (Year-to-Date)" if is_daily else "Current Month"
        
        content = f"""
<h1>Task Usage Report{title_suffix}{' - TEST DATA' if test_mode else ''}</h1>
<p><strong>Last updated:</strong> {timestamp}</p>
<p><strong>Generated by:</strong> {user}</p>
<p><strong>Data through:</strong> {yesterday} (excludes today)</p>
<p><strong>Report Mode:</strong> {date_range_desc}</p>
<h2>Overall Monthly Task Usage Report</h2>
{jobs_data_table}
{peaks_section}
{monthly_usage_chart}
{monthly_country_chart}
{daily_chart}
{country_chart}
<hr />
<p><em>Note: This report shows the task usage data through {yesterday} in {date_range_desc} mode{' (TEST MODE)' if test_mode else ''}.</em></p>
"""

        # Skip actual upload if requested
        if skip_actual_upload:
            logging.info("Skipping actual upload (simulation mode)")
            return True

        # Get page info and create/update
        page_id, version = get_page_info(session, config)
        return create_or_update_page(session, config, content, page_id, version)

    except Exception as e:
        logging.error(f"Error in publish_to_confluence: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return False

# =============================================================
# CLI Entry Point
# =============================================================

if __name__ == "__main__":
    import argparse
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    
    p = argparse.ArgumentParser(description="Confluence Publisher")
    p.add_argument("--file", help="Single report CSV to publish")
    p.add_argument("--simulate", action="store_true", help="Skip actual upload")
    p.add_argument("--test", action="store_true", help="Test mode")
    p.add_argument("--daily", action="store_true", help="Force daily/YTD mode")
    p.add_argument("--monthly", action="store_true", help="Force monthly mode")
    args = p.parse_args()
    
    # Determine mode from command line flags - DEFAULT TO MONTHLY IF NOT SPECIFIED
    is_daily_mode = False  # Default to monthly
    if args.daily:
        is_daily_mode = True
    elif args.monthly:
        is_daily_mode = False
    
    if args.file:
        ok = publish_to_confluence(
            report_file=args.file,
            test_mode=args.test,
            skip_actual_upload=args.simulate,
            is_daily_mode=is_daily_mode
        )
        print("Single publish:", "OK" if ok else "FAILED")
    else:
        p.print_help()
