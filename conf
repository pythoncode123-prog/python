import requests
from requests.auth import HTTPBasicAuth
import pandas as pd
import json
import urllib3
import os
from datetime import datetime
import logging
import re
from typing import Tuple, Optional, Dict, List

# -------------------------------------------------------------
# Confluence Publisher - Complete Final Version with Fixes
# -------------------------------------------------------------

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

MONTH_NAMES = (
    "January", "February", "March", "April", "May", "June",
    "July", "August", "September", "October", "November", "December"
)

# =============================================================
# Configuration / Auth
# =============================================================

def _is_monthly_run(config: Dict) -> bool:
    title = config.get('PAGE_TITLE', '')
    if title.endswith('_daily'):
        return False
    if re.search(r'\s-\s(' + '|'.join(MONTH_NAMES) + r')$', title):
        return True
    return True  # default behave as monthly

def find_config_file(test_mode: bool = False) -> Optional[str]:
    """
    Find the config file in various possible locations.
    Returns the path to the first existing config file found.
    """
    # Define search paths based on mode
    if test_mode:
        search_paths = [
            "config/config_test.json",
            "config_test.json",
            "config/config.json",
            "config.json"
        ]
    else:
        search_paths = [
            "config/config.json",
            "config.json",
            "config/config_test.json",
            "config_test.json"
        ]
    
    # Check each path
    for path in search_paths:
        abs_path = os.path.abspath(path)
        if os.path.exists(abs_path):
            logging.info(f"Found config file at: {abs_path}")
            return abs_path
    
    # Log all attempted paths if none found
    logging.error(f"Config file not found. Searched in: {search_paths}")
    return None

def load_config(config_path: str = None) -> Optional[Dict]:
    """
    Load configuration from the specified or auto-detected path.
    """
    try:
        # If no path specified, try to find it
        if not config_path:
            config_path = find_config_file()
            if not config_path:
                logging.error("Could not find config file in any expected location")
                return None
        
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        required = ['CONFLUENCE_URL', 'USERNAME', 'SPACE_KEY', 'PAGE_TITLE']
        if not all(k in config for k in required):
            logging.error(f"Missing required configuration keys. Need: {required}")
            return None
        
        if 'BASELINE' not in config:
            config['BASELINE'] = 1899206
        
        logging.info(f"Successfully loaded config from {config_path}")
        return config
    except Exception as e:
        logging.error(f"Error loading config from {config_path}: {e}")
        return None

def get_password_from_config(config: Dict) -> Optional[str]:
    """
    Get password from config - either encrypted or base64 encoded.
    """
    # Try encrypted password first
    if 'PASSWORD_ENCRYPTED' in config:
        try:
            from lib.secure_config import SecureConfig
            password = SecureConfig.decrypt_password(config['PASSWORD_ENCRYPTED'])
            if password:
                return password
        except Exception as e:
            logging.warning(f"Could not decrypt password: {e}")
    
    # Try base64 encoded password
    if 'PASSWORD_B64' in config:
        try:
            import base64
            password = base64.b64decode(config['PASSWORD_B64']).decode('utf-8')
            if password:
                return password
        except Exception as e:
            logging.warning(f"Could not decode base64 password: {e}")
    
    # Try plain password (not recommended)
    if 'PASSWORD' in config:
        return config['PASSWORD']
    
    return None

def create_session(config: Dict) -> Optional[requests.Session]:
    """
    Create authenticated session for Confluence API.
    """
    session = requests.Session()
    auth_type = config.get('AUTH_TYPE', 'basic').lower()
    
    if auth_type == 'basic':
        # Try various password sources
        password = (os.environ.get('CONFLUENCE_PASSWORD') or
                    get_password_from_config(config) or
                    config.get('API_TOKEN'))
        
        if not password:
            logging.error("Missing credentials for basic auth.")
            return None
        
        session.auth = HTTPBasicAuth(config['USERNAME'], password)
        
    elif auth_type == 'jwt':
        token = os.environ.get('CONFLUENCE_TOKEN') or config.get('API_TOKEN')
        if not token:
            logging.error("Missing JWT token.")
            return None
        session.headers.update({"Authorization": f"Bearer {token}"})
        
    elif auth_type == 'cookie':
        cookie = os.environ.get('CONFLUENCE_COOKIE') or config.get('SESSION_COOKIE')
        if not cookie:
            logging.error("Missing cookie value for cookie auth.")
            return None
        session.headers.update({"Cookie": cookie})
    else:
        logging.error(f"Unsupported AUTH_TYPE {auth_type}")
        return None

    session.headers.update({
        "Content-Type": "application/json",
        "X-Atlassian-Token": "no-check"
    })
    
    if 'PROXY' in config:
        session.proxies = {"http": config['PROXY'], "https": config['PROXY']}
    
    session.verify = False
    return session

# =============================================================
# Data Loading & Sanitization
# =============================================================

def load_csv_data(csv_file: str) -> Tuple[pd.DataFrame, bool]:
    try:
        df = pd.read_csv(csv_file)
        if 'DATE' in df.columns:
            df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')
        return df, True
    except Exception as e:
        logging.error(f"Error loading CSV {csv_file}: {e}")
        return pd.DataFrame(), False

def sanitize_total_jobs(df: pd.DataFrame,
                        preferred_col: Optional[str] = None) -> pd.DataFrame:
    """
    Ensure we have a numeric TOTAL_JOBS column.
    """
    candidate = None
    if preferred_col and preferred_col in df.columns:
        candidate = preferred_col
    elif 'TOTAL_JOBS' in df.columns:
        candidate = 'TOTAL_JOBS'
    else:
        # Try to guess a column with large ints
        for c in df.columns:
            if re.search(r'jobs|count|total', c.lower()):
                candidate = c
                break

    if candidate is None:
        logging.error("No suitable column found for TOTAL_JOBS.")
        df['TOTAL_JOBS'] = pd.Series(dtype='float')
        return df

    # Create/overwrite TOTAL_JOBS from candidate
    cleaned = (
        df[candidate]
        .astype(str)
        .str.replace(',', '', regex=False)
        .str.replace(' ', '', regex=False)
        .str.strip()
    )
    df['TOTAL_JOBS'] = pd.to_numeric(cleaned, errors='coerce')

    return df

# =============================================================
# Peak Calculation - UPDATED FOR DAILY/MONTHLY
# =============================================================

def latest_year_month(daily_df: pd.DataFrame) -> Tuple[int, int]:
    latest = daily_df['DATE'].max()
    return latest.year, latest.month

def filter_to_month(daily_df: pd.DataFrame,
                    year: Optional[int],
                    month: Optional[int]) -> pd.DataFrame:
    if year is None or month is None:
        year, month = latest_year_month(daily_df)
    result = daily_df[(daily_df.DATE.dt.year == year) & (daily_df.DATE.dt.month == month)].copy()
    return result, year, month

def compute_top_peaks(daily_df: pd.DataFrame,
                      top_n: int,
                      year: Optional[int],
                      month: Optional[int]) -> Tuple[pd.DataFrame, int, int]:
    month_df, y, m = filter_to_month(daily_df, year, month)
    if month_df.empty:
        return month_df, y, m
    try:
        peaks = month_df.nlargest(top_n, 'TOTAL_JOBS')
    except Exception:
        peaks = month_df.sort_values('TOTAL_JOBS', ascending=False).head(top_n)
    return peaks[['DATE', 'TOTAL_JOBS']], y, m

def compute_top_peaks_ytd(daily_df: pd.DataFrame, top_n: int = 4) -> pd.DataFrame:
    """Compute top N peaks for Year-To-Date (for daily reports)"""
    if daily_df.empty:
        return daily_df
    
    # Get current year
    current_year = datetime.now().year
    
    # Filter to current year
    ytd_df = daily_df[daily_df['DATE'].dt.year == current_year].copy()
    
    if ytd_df.empty:
        # Fallback to all data if no current year data
        ytd_df = daily_df.copy()
    
    # Get top N peaks by total jobs
    try:
        peaks = ytd_df.nlargest(top_n, 'TOTAL_JOBS')
    except Exception:
        peaks = ytd_df.sort_values('TOTAL_JOBS', ascending=False).head(top_n)
    
    return peaks[['DATE', 'TOTAL_JOBS']].sort_values('DATE')

# =============================================================
# Chart Builders - UPDATED WITH FIXES
# =============================================================

def peaks_transposed_chart(peaks_df: pd.DataFrame,
                           baseline: int,
                           chart_title: str) -> str:
    """
    Create chart with transposed data (dates as columns, series as rows)
    Using specific color codes: red #B83939 for baseline, green #829A3D for total jobs
    """
    if peaks_df.empty:
        return f"<p>No data for {chart_title}.</p>"
    
    # Sort by date
    peaks_df = peaks_df.sort_values('DATE')
    
    # Format dates as MM-DD for cleaner display
    date_format = peaks_df['DATE'].dt.strftime('%m-%d')
    
    # First row: Date headers
    header_row = "<tr><th>Date</th>"
    for date in date_format:
        header_row += f"<th>{date}</th>"
    header_row += "</tr>"
    
    # Second row: Baseline values
    baseline_row = "<tr><td>Baseline</td>"
    for _ in range(len(peaks_df)):
        baseline_row += f"<td>{int(baseline)}</td>"
    baseline_row += "</tr>"
    
    # Third row: Total Jobs values
    jobs_row = "<tr><td>Total Jobs</td>"
    for _, row in peaks_df.iterrows():
        jobs_row += f"<td>{int(row['TOTAL_JOBS'])}</td>"
    jobs_row += "</tr>"
    
    return f"""
<ac:structured-macro ac:name="chart">
  <ac:parameter ac:name="title">{chart_title}</ac:parameter>
  <ac:parameter ac:name="type">bar</ac:parameter>
  <ac:parameter ac:name="orientation">vertical</ac:parameter>
  <ac:parameter ac:name="3D">true</ac:parameter>
  <ac:parameter ac:name="width">600</ac:parameter>
  <ac:parameter ac:name="height">400</ac:parameter>
  <ac:parameter ac:name="legend">true</ac:parameter>
  <ac:parameter ac:name="dataDisplay">true</ac:parameter>
  <ac:parameter ac:name="stacked">false</ac:parameter>
  <ac:parameter ac:name="showValues">true</ac:parameter>
  <ac:parameter ac:name="colors">#B83939,#829A3D</ac:parameter>
  <ac:rich-text-body>
    <table>
      <tbody>
        {header_row}
        {baseline_row}
        {jobs_row}
      </tbody>
    </table>
  </ac:rich-text-body>
</ac:structured-macro>
"""

def peaks_detail_table(peaks_df: pd.DataFrame,
                       baseline: int,
                       label: str) -> str:
    if peaks_df.empty:
        return ""
    ranked = peaks_df.sort_values('TOTAL_JOBS', ascending=False).reset_index(drop=True)
    rank_map = {row.DATE: idx + 1 for idx, row in ranked.iterrows()}
    display = peaks_df.sort_values('DATE').copy()
    display['Variation'] = baseline - display['TOTAL_JOBS']
    rows = ["<tr><th>Rank</th><th>Date</th><th>Total Jobs</th><th>Baseline</th><th>Variation</th></tr>"]
    for _, r in display.iterrows():
        variation = int(r['Variation'])
        style = 'style="background-color:#90EE90;"' if variation > 0 else 'style="background-color:#FFB6C1;"'
        rows.append(
            f"<tr><td>{rank_map[r['DATE']]}</td>"
            f"<td>{r['DATE'].strftime('%Y-%m-%d')}</td>"
            f"<td>{int(r['TOTAL_JOBS'])}</td>"
            f"<td>{baseline}</td>"
            f"<td {style}>{variation}</td></tr>"
        )
    return f"<h4>Top 4 Peaks Detail ({label})</h4><table class='wrapped'><tbody>{''.join(rows)}</tbody></table>"

def generate_jobs_data_table(df: pd.DataFrame, baseline: int, is_daily: bool = False) -> str:
    """
    Generate the Jobs Data table showing ONLY TOP 4 PEAKS with Date, Baseline, Total Jobs, and Variation.
    FIXED: Variance = Baseline - Total Jobs (positive when baseline is higher)
    For daily: YTD top 4 peaks
    For monthly: Current month top 4 peaks
    """
    # Group by date and sum total jobs
    daily = df.groupby('DATE', as_index=False)['TOTAL_JOBS'].sum()
    
    if daily.empty:
        return "<p>No daily data available.</p>"
    
    # Get top 4 peaks based on mode
    if is_daily:
        top_4_peaks = compute_top_peaks_ytd(daily, 4)
        title = "Jobs Data - Top 4 Peaks (YTD)"
    else:
        top_4_peaks, _, _ = compute_top_peaks(daily, 4, None, None)
        title = "Jobs Data - Top 4 Peaks"
    
    if top_4_peaks.empty:
        return f"<p>No peak data available for {title}</p>"
    
    # Start the table
    html = f"""
<h3>{title}</h3>
<table class='wrapped'>
<tbody>
<tr>
    <th>Date</th>
    <th>Baseline</th>
    <th>Total Jobs</th>
    <th>Variation</th>
</tr>
"""
    
    # Add data rows for top 4 only
    for _, row in top_4_peaks.iterrows():
        date_str = row['DATE'].strftime('%m/%d/%Y')
        total_jobs = int(row['TOTAL_JOBS'])
        variation = baseline - total_jobs  # FIXED: Baseline - Total Jobs as requested
        
        # Color coding logic - positive (baseline higher) = green, negative (total jobs higher) = red
        if variation > 0:
            var_style = 'style="background-color:#90EE90;"'  # Light green for positive
            var_text = f"{variation:,}"
        elif variation < 0:
            var_style = 'style="background-color:#FFB6C1;"'  # Light red for negative
            var_text = f"{variation:,}"
        else:
            var_style = ''
            var_text = "0"
        
        html += f"""
<tr>
    <td>{date_str}</td>
    <td>{baseline:,}</td>
    <td>{total_jobs:,}</td>
    <td {var_style}>{var_text}</td>
</tr>
"""
    
    html += "</tbody></table>"
    return html

def generate_global_peaks_section(df: pd.DataFrame,
                                  baseline: int,
                                  top_n: int = 4,
                                  is_daily: bool = False) -> str:
    daily = df.groupby('DATE', as_index=False)['TOTAL_JOBS'].sum()
    
    if is_daily:
        peaks = compute_top_peaks_ytd(daily, top_n)
        chart_title = "4th Peak of YTD"
    else:
        peaks, y, m = compute_top_peaks(daily, top_n, None, None)
        if peaks.empty:
            return "<p>No peak data for current month.</p>"
        month_label = peaks['DATE'].dt.strftime('%b').iloc[0]
        chart_title = f"Top {len(peaks)} Peaks of {month_label}"
    
    if peaks.empty:
        return "<p>No peak data available.</p>"
        
    chart = peaks_transposed_chart(peaks, baseline, chart_title)
    return chart

def generate_monthly_usage_chart(df: pd.DataFrame, baseline: int) -> str:
    """
    Generate Overall Monthly Task Usage Report line chart
    Shows trend over time with baseline comparison
    """
    # Group by date and sum
    daily = df.groupby('DATE', as_index=False)['TOTAL_JOBS'].sum().sort_values('DATE')
    
    if daily.empty:
        return "<p>No data for monthly usage chart.</p>"
    
    # Build the data table for the chart
    header_row = "<tr><th>Date</th>"
    for _, row in daily.iterrows():
        date_str = row['DATE'].strftime('%Y-%m-%d')
        header_row += f"<th>{date_str}</th>"
    header_row += "</tr>"
    
    # Baseline row
    baseline_row = "<tr><td>Baseline</td>"
    for _ in range(len(daily)):
        baseline_row += f"<td>{baseline}</td>"
    baseline_row += "</tr>"
    
    # Total Jobs row
    jobs_row = "<tr><td>Sum of TOTAL_JOBS</td>"
    for _, row in daily.iterrows():
        jobs_row += f"<td>{int(row['TOTAL_JOBS'])}</td>"
    jobs_row += "</tr>"
    
    return f"""
<h3>Overall Monthly Task Usage Report</h3>
<ac:structured-macro ac:name="chart">
  <ac:parameter ac:name="title">Overall Monthly Task Usage Report</ac:parameter>
  <ac:parameter ac:name="type">line</ac:parameter>
  <ac:parameter ac:name="width">1200</ac:parameter>
  <ac:parameter ac:name="height">400</ac:parameter>
  <ac:parameter ac:name="3D">false</ac:parameter>
  <ac:parameter ac:name="legend">true</ac:parameter>
  <ac:parameter ac:name="showValues">false</ac:parameter>
  <ac:parameter ac:name="colors">#829A3D,#B83939</ac:parameter>
  <ac:parameter ac:name="dataDisplay">after</ac:parameter>
  <ac:parameter ac:name="xLabel">Date</ac:parameter>
  <ac:parameter ac:name="yLabel">Jobs</ac:parameter>
  <ac:rich-text-body>
    <table>
      <tbody>
        {header_row}
        {baseline_row}
        {jobs_row}
      </tbody>
    </table>
  </ac:rich-text-body>
</ac:structured-macro>
"""

def generate_daily_total_single_series(df: pd.DataFrame,
                                       baseline: int,
                                       title: str = "Daily Total Jobs") -> str:
    """
    Generate daily totals chart with single green color.
    FIXED: Made X-axis labels vertical and increased chart height for better visibility
    """
    daily = df.groupby('DATE', as_index=False)['TOTAL_JOBS'].sum().sort_values('DATE')
    if daily.empty:
        return "<p>No daily data.</p>"
    
    # Convert to the format needed for the chart
    date_columns = {}
    for _, row in daily.iterrows():
        # Use DD-MM format for better readability and to avoid *** display
        date_str = row['DATE'].strftime('%d-%m')  # Changed to DD-MM format
        total_jobs = int(row['TOTAL_JOBS'])
        date_columns[date_str] = total_jobs
    
    # Create header row
    header_row = "<tr><th>Series</th>"
    for date in date_columns.keys():
        header_row += f"<th>{date}</th>"
    header_row += "</tr>"
    
    # Create data row
    data_row = "<tr><td>Total Jobs</td>"
    for value in date_columns.values():
        data_row += f"<td>{value}</td>"
    data_row += "</tr>"
    
    table_html = f"""<table><tbody>{header_row}{data_row}</tbody></table>"""
    
    chart = f"""
<h3>{title}</h3>
<ac:structured-macro ac:name="chart">
  <ac:parameter ac:name="type">bar</ac:parameter>
  <ac:parameter ac:name="title">{title}</ac:parameter>
  <ac:parameter ac:name="width">1200</ac:parameter>
  <ac:parameter ac:name="height">600</ac:parameter>
  <ac:parameter ac:name="3D">true</ac:parameter>
  <ac:parameter ac:name="legend">false</ac:parameter>
  <ac:parameter ac:name="orientation">vertical</ac:parameter>
  <ac:parameter ac:name="labelAngle">90</ac:parameter>
  <ac:parameter ac:name="stacked">false</ac:parameter>
  <ac:parameter ac:name="showValues">true</ac:parameter>
  <ac:parameter ac:name="xaxisLabel">Date</ac:parameter>
  <ac:parameter ac:name="yaxisLabel">Jobs</ac:parameter>
  <ac:parameter ac:name="colors">#829A3D</ac:parameter>
  <ac:parameter ac:name="dataDisplay">after</ac:parameter>
  <ac:parameter ac:name="fontSize">10</ac:parameter>
  <ac:rich-text-body>
    {table_html}
  </ac:rich-text-body>
</ac:structured-macro>
"""
    return chart

def generate_country_summary_chart(df: pd.DataFrame,
                                  title: str = "Total Jobs by Country") -> str:
    """
    Generate bar chart showing total jobs per country with summary table.
    """
    # Check if we have country data - try multiple possible column names
    country_col = None
    possible_country_cols = ['COUNTRY', 'Country', 'country', 'REGION', 'Region', 'region']
    
    for col in possible_country_cols:
        if col in df.columns:
            country_col = col
            break
    
    if country_col is None:
        return "<p>No country data available for country summary chart.</p>"
    
    # Group by country and sum total jobs
    country_totals = df.groupby(country_col, as_index=False)['TOTAL_JOBS'].sum()
    country_totals = country_totals.sort_values('TOTAL_JOBS', ascending=False)
    
    if country_totals.empty:
        return "<p>No country data available.</p>"
    
    # Create summary table
    table_html = f"""
<h4>Country Summary</h4>
<table class='wrapped'>
<tbody>
<tr>
    <th>Country/Market</th>
    <th>Total Jobs</th>
    <th>Percentage</th>
</tr>
"""
    
    total_all_jobs = country_totals['TOTAL_JOBS'].sum()
    
    for _, row in country_totals.iterrows():
        country = row[country_col]
        jobs = int(row['TOTAL_JOBS'])
        percentage = (jobs / total_all_jobs * 100) if total_all_jobs > 0 else 0
        table_html += f"""
<tr>
    <td>{country}</td>
    <td>{jobs:,}</td>
    <td>{percentage:.1f}%</td>
</tr>
"""
    
    table_html += "</tbody></table>"
    
    # Create chart table data
    chart_header = "<tr><th>Country/Market</th><th>Task Count</th></tr>"
    chart_rows = []
    for _, row in country_totals.iterrows():
        country = row[country_col]
        jobs = int(row['TOTAL_JOBS'])
        chart_rows.append(f"<tr><td>{country}</td><td>{jobs}</td></tr>")
    
    chart_table_html = f"""<table><tbody>{chart_header}{''.join(chart_rows)}</tbody></table>"""
    
    chart = f"""
<h3>{title}</h3>
<ac:structured-macro ac:name="chart">
  <ac:parameter ac:name="type">bar</ac:parameter>
  <ac:parameter ac:name="title">{title}</ac:parameter>
  <ac:parameter ac:name="width">1200</ac:parameter>
  <ac:parameter ac:name="height">500</ac:parameter>
  <ac:parameter ac:name="3D">true</ac:parameter>
  <ac:parameter ac:name="legend">false</ac:parameter>
  <ac:parameter ac:name="orientation">vertical</ac:parameter>
  <ac:parameter ac:name="stacked">false</ac:parameter>
  <ac:parameter ac:name="showValues">true</ac:parameter>
  <ac:parameter ac:name="xaxisLabel">Country/Market</ac:parameter>
  <ac:parameter ac:name="yaxisLabel">Task Count</ac:parameter>
  <ac:parameter ac:name="colors">#829A3D</ac:parameter>
  <ac:parameter ac:name="dataDisplay">after</ac:parameter>
  <ac:parameter ac:name="labelAngle">45</ac:parameter>
  <ac:rich-text-body>
    {chart_table_html}
  </ac:rich-text-body>
</ac:structured-macro>
"""
    
    return table_html + chart

# =============================================================
# Confluence REST Helpers
# =============================================================

def get_page_info(session: requests.Session, config: Dict) -> Tuple[Optional[str], Optional[int]]:
    try:
        base_url = config['CONFLUENCE_URL'].rstrip('/')
        if not base_url.endswith('/content'):
            if base_url.endswith('/rest/api'):
                base_url = base_url + "/content"
            else:
                base_url = base_url + "/rest/api/content"
        
        params = {
            'title': config['PAGE_TITLE'],
            'spaceKey': config['SPACE_KEY'],
            'expand': 'version'
        }
        resp = session.get(base_url, params=params)
        
        if resp.status_code != 200:
            alt = f"{base_url}/search?cql=space={config['SPACE_KEY']} AND title=\"{config['PAGE_TITLE']}\""
            resp = session.get(alt)
        
        if resp.status_code != 200:
            logging.error(f"Page lookup failed: {resp.status_code} - {resp.text}")
            return None, None
        
        data = resp.json()
        if 'results' in data and data.get('size', 0) > 0:
            return data['results'][0]['id'], data['results'][0]['version']['number']
        if isinstance(data, list) and data:
            return data[0]['id'], data[0]['version']['number']
        return None, None
    except Exception as e:
        logging.error(f"Error getting page info: {e}")
        return None, None

def create_or_update_page(session: requests.Session,
                          config: Dict,
                          content: str,
                          page_id: Optional[str] = None,
                          version: Optional[int] = None) -> bool:
    try:
        base_url = config['CONFLUENCE_URL'].rstrip('/')
        if not base_url.endswith('/content'):
            if base_url.endswith('/rest/api'):
                base_url = base_url + "/content"
            else:
                base_url = base_url + "/rest/api/content"
        
        payload = {
            "type": "page",
            "title": config['PAGE_TITLE'],
            "space": {"key": config['SPACE_KEY']},
            "body": {"storage": {"value": content, "representation": "storage"}}
        }
        
        if page_id and version is not None:
            payload["id"] = page_id
            payload["version"] = {"number": version + 1}
            resp = session.put(f"{base_url}/{page_id}", json=payload)
        else:
            resp = session.post(base_url, json=payload)
        
        if resp.status_code >= 400:
            logging.error(f"Create/Update failed: {resp.status_code} - {resp.text}")
            return False
        
        logging.info(f"Page '{config['PAGE_TITLE']}' {'updated' if page_id else 'created'} successfully.")
        return True
    except Exception as e:
        logging.error(f"Error creating/updating page: {e}")
        return False

def test_connection(session: requests.Session, config: Dict) -> bool:
    try:
        base = config['CONFLUENCE_URL'].rstrip('/')
        if base.endswith('/content'):
            base = base[:-8]
        elif not base.endswith('/rest/api'):
            base = base + '/rest/api'
        test_url = f"{base}/space"
        resp = session.get(test_url, params={'limit': 1})
        return resp.status_code == 200
    except Exception as e:
        logging.error(f"Connection test exception: {e}")
        return False

# =============================================================
# Main Publish Function - UPDATED
# =============================================================

def publish_to_confluence(report_file='task_usage_report_by_region.csv',
                          test_mode=False,
                          skip_actual_upload: bool = False) -> bool:
    """
    Publish report to Confluence with improved config file handling.
    """
    try:
        # Check report file exists
        if not os.path.exists(report_file):
            logging.error(f"Report file {report_file} not found.")
            return False
        
        # Find and load config
        cfg_path = find_config_file(test_mode)
        if not cfg_path:
            logging.error("Could not find config file")
            return False
        
        config = load_config(cfg_path)
        if not config:
            logging.error("Failed to load config")
            return False
        
        # Add test suffix if in test mode
        if test_mode and not config['PAGE_TITLE'].endswith("-TEST"):
            config['PAGE_TITLE'] += "-TEST"

        monthly = _is_monthly_run(config)
        is_daily = not monthly
        baseline = config.get('BASELINE', 1899206)

        # Load and sanitize data
        df, ok = load_csv_data(report_file)
        if not ok:
            logging.error("Failed to load CSV data")
            return False
        
        df = sanitize_total_jobs(df, preferred_col=config.get('JOB_COLUMN'))

        # Create session if not skipping upload
        session = None
        if not skip_actual_upload:
            session = create_session(config)
            if not session:
                logging.error("Failed to create session")
                return False
            
            if not test_connection(session, config):
                logging.error("Connection test failed.")
                return False

        # Generate content sections - UPDATED FOR DAILY/MONTHLY
        peaks_section = generate_global_peaks_section(df, baseline, is_daily=is_daily) if monthly or is_daily else ""
        jobs_data_table = generate_jobs_data_table(df, baseline, is_daily=is_daily)
        monthly_usage_chart = generate_monthly_usage_chart(df, baseline)
        daily_chart = generate_daily_total_single_series(df, baseline, "Daily Total Jobs")
        country_chart = generate_country_summary_chart(df, "Total Jobs by Country")

        # Create page content
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        user = os.environ.get('USER', 'automated')
        
        title_suffix = " - YTD" if is_daily else ""
        
        content = f"""
<h1>Task Usage Report{title_suffix}{' - TEST DATA' if test_mode else ''}</h1>
<p><strong>Last updated:</strong> {timestamp}</p>
<p><strong>Generated by:</strong> {user}</p>
<h2>Overall Monthly Task Usage Report</h2>
{jobs_data_table}
{peaks_section}
{monthly_usage_chart}
{daily_chart}
{country_chart}
<hr />
<p><em>Note: This report shows the task usage data{' (TEST MODE)' if test_mode else ''}.</em></p>
"""

        # Skip actual upload if requested
        if skip_actual_upload:
            logging.info("Skipping actual upload (simulation mode)")
            return True

        # Get page info and create/update
        page_id, version = get_page_info(session, config)
        return create_or_update_page(session, config, content, page_id, version)

    except Exception as e:
        logging.error(f"Error in publish_to_confluence: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return False

# =============================================================
# Multi-Country Publishing
# =============================================================

def publish_to_confluence_multi(report_files: List[Tuple[str, str]],
                                test_mode: bool = False,
                                skip_actual_upload: bool = False) -> bool:
    """
    Publish multi-country report to Confluence.
    """
    try:
        # Find and load config
        cfg_path = find_config_file(test_mode)
        if not cfg_path:
            logging.error("Could not find config file for multi-country")
            return False
        
        config = load_config(cfg_path)
        if not config:
            logging.error("Failed to load config for multi-country")
            return False
        
        if test_mode and not config['PAGE_TITLE'].endswith("-TEST"):
            config['PAGE_TITLE'] += "-TEST"

        monthly = _is_monthly_run(config)
        is_daily = not monthly
        baseline = config.get('BASELINE', 1899206)

        # Process all country files
        frames = []
        for ctry, path in report_files:
            if not os.path.exists(path):
                logging.warning(f"Missing report for {ctry}: {path}")
                continue
            df = pd.read_csv(path)
            if 'DATE' in df.columns:
                df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')
            df = sanitize_total_jobs(df, preferred_col=config.get('JOB_COLUMN'))
            df['COUNTRY'] = ctry
            frames.append(df)

        if not frames:
            logging.error("No country data to publish.")
            return False

        all_df = pd.concat(frames, ignore_index=True)

        # Create session if not skipping
        session = None
        if not skip_actual_upload:
            session = create_session(config)
            if not session:
                return False
            if not test_connection(session, config):
                logging.error("Connection test failed.")
                return False

        # Generate content - UPDATED FOR DAILY/MONTHLY
        global_peaks = generate_global_peaks_section(all_df, baseline, is_daily=is_daily) if monthly or is_daily else ""
        jobs_data_table = generate_jobs_data_table(all_df, baseline, is_daily=is_daily)
        monthly_usage_chart = generate_monthly_usage_chart(all_df, baseline)
        global_daily = generate_daily_total_single_series(all_df, baseline, "Daily Total Jobs (All Countries)")
        country_chart = generate_country_summary_chart(all_df, "Total Jobs by Country")

        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        user = os.environ.get('USER', 'automated')
        
        title_suffix = " - YTD" if is_daily else ""
        
        content = f"""
<h1>Multi-Country Task Usage Report{title_suffix}{' - TEST DATA' if test_mode else ''}</h1>
<p><strong>Last updated:</strong> {timestamp}</p>
<p><strong>Generated by:</strong> {user}</p>
<h2>Overall Monthly Task Usage Report</h2>
{jobs_data_table}
{global_peaks}
{monthly_usage_chart}
{global_daily}
{country_chart}
<hr />
<p><em>Note: This report shows aggregated data from multiple countries{' (TEST MODE)' if test_mode else ''}.</em></p>
"""

        if skip_actual_upload:
            logging.info("Simulation only (multi-country).")
            return True

        page_id, version = get_page_info(session, config)
        return create_or_update_page(session, config, content, page_id, version)

    except Exception as e:
        logging.error(f"Error in multi-country publish: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return False

# =============================================================
# CLI Entry Point
# =============================================================

if __name__ == "__main__":
    import argparse
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    
    p = argparse.ArgumentParser(description="Confluence Publisher")
    p.add_argument("--file", help="Single report CSV to publish")
    p.add_argument("--multi", nargs="*", help="List of COUNTRY=path/to/file.csv")
    p.add_argument("--simulate", action="store_true", help="Skip actual upload")
    p.add_argument("--test", action="store_true", help="Test mode")
    args = p.parse_args()
    
    if args.file:
        ok = publish_to_confluence(
            report_file=args.file,
            test_mode=args.test,
            skip_actual_upload=args.simulate
        )
        print("Single publish:", "OK" if ok else "FAILED")
    elif args.multi:
        pairs = []
        for item in args.multi:
            if "=" not in item:
                print(f"Invalid format: {item}")
                continue
            ctry, path = item.split("=", 1)
            pairs.append((ctry.strip(), path.strip()))
        ok = publish_to_confluence_multi(
            report_files=pairs,
            test_mode=args.test,
            skip_actual_upload=args.simulate
        )
        print("Multi publish:", "OK" if ok else "FAILED")
    else:
        p.print_help()
