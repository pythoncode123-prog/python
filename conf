import requests
from requests.auth import HTTPBasicAuth
import pandas as pd
import json
import urllib3
import os
from datetime import datetime
import logging
import re
from typing import Tuple, Optional, Dict, List

# -------------------------------------------------------------
# Confluence Publisher - Complete Final Version with Fixes
# -------------------------------------------------------------

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

MONTH_NAMES = (
    "January", "February", "March", "April", "May", "June",
    "July", "August", "September", "October", "November", "December"
)

# =============================================================
# Configuration / Auth
# =============================================================

def _is_monthly_run(config: Dict) -> bool:
    title = config.get('PAGE_TITLE', '')
    if title.endswith('_daily'):
        return False
    if re.search(r'\s-\s(' + '|'.join(MONTH_NAMES) + r')$', title):
        return True
    return True  # default behave as monthly

def find_config_file(test_mode: bool = False) -> Optional[str]:
    """
    Find the config file in various possible locations.
    Returns the path to the first existing config file found.
    """
    # Define search paths based on mode
    if test_mode:
        search_paths = [
            "config/config_test.json",
            "config_test.json",
            "config/config.json",
            "config.json"
        ]
    else:
        search_paths = [
            "config/config.json",
            "config.json",
            "config/config_test.json",
            "config_test.json"
        ]
    
    # Check each path
    for path in search_paths:
        abs_path = os.path.abspath(path)
        if os.path.exists(abs_path):
            logging.info(f"Found config file at: {abs_path}")
            return abs_path
    
    # Log all attempted paths if none found
    logging.error(f"Config file not found. Searched in: {search_paths}")
    return None

def load_config(config_path: str = None) -> Optional[Dict]:
    """
    Load configuration from the specified or auto-detected path.
    """
    try:
        # If no path specified, try to find it
        if not config_path:
            config_path = find_config_file()
            if not config_path:
                logging.error("Could not find config file in any expected location")
                return None
        
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        required = ['CONFLUENCE_URL', 'USERNAME', 'SPACE_KEY', 'PAGE_TITLE']
        if not all(k in config for k in required):
            logging.error(f"Missing required configuration keys. Need: {required}")
            return None
        
        if 'BASELINE' not in config:
            config['BASELINE'] = 1899206
        
        logging.info(f"Successfully loaded config from {config_path}")
        return config
    except Exception as e:
        logging.error(f"Error loading config from {config_path}: {e}")
        return None

def get_password_from_config(config: Dict) -> Optional[str]:
    """
    Get password from config - either encrypted or base64 encoded.
    """
    # Try encrypted password first
    if 'PASSWORD_ENCRYPTED' in config:
        try:
            from lib.secure_config import SecureConfig
            password = SecureConfig.decrypt_password(config['PASSWORD_ENCRYPTED'])
            if password:
                return password
        except Exception as e:
            logging.warning(f"Could not decrypt password: {e}")
    
    # Try base64 encoded password
    if 'PASSWORD_B64' in config:
        try:
            import base64
            password = base64.b64decode(config['PASSWORD_B64']).decode('utf-8')
            if password:
                return password
        except Exception as e:
            logging.warning(f"Could not decode base64 password: {e}")
    
    # Try plain password (not recommended)
    if 'PASSWORD' in config:
        return config['PASSWORD']
    
    return None

def create_session(config: Dict) -> Optional[requests.Session]:
    """
    Create authenticated session for Confluence API.
    """
    session = requests.Session()
    auth_type = config.get('AUTH_TYPE', 'basic').lower()
    
    if auth_type == 'basic':
        # Try various password sources
        password = (os.environ.get('CONFLUENCE_PASSWORD') or
                    get_password_from_config(config) or
                    config.get('API_TOKEN'))
        
        if not password:
            logging.error("Missing credentials for basic auth.")
            return None
        
        session.auth = HTTPBasicAuth(config['USERNAME'], password)
        
    elif auth_type == 'jwt':
        token = os.environ.get('CONFLUENCE_TOKEN') or config.get('API_TOKEN')
        if not token:
            logging.error("Missing JWT token.")
            return None
        session.headers.update({"Authorization": f"Bearer {token}"})
        
    elif auth_type == 'cookie':
        cookie = os.environ.get('CONFLUENCE_COOKIE') or config.get('SESSION_COOKIE')
        if not cookie:
            logging.error("Missing cookie value for cookie auth.")
            return None
        session.headers.update({"Cookie": cookie})
    else:
        logging.error(f"Unsupported AUTH_TYPE {auth_type}")
        return None

    session.headers.update({
        "Content-Type": "application/json",
        "X-Atlassian-Token": "no-check"
    })
    
    if 'PROXY' in config:
        session.proxies = {"http": config['PROXY'], "https": config['PROXY']}
    
    session.verify = False
    return session

# =============================================================
# Data Loading & Sanitization
# =============================================================

def load_csv_data(csv_file: str) -> Tuple[pd.DataFrame, bool]:
    try:
        df = pd.read_csv(csv_file)
        if 'DATE' in df.columns:
            df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')
        return df, True
    except Exception as e:
        logging.error(f"Error loading CSV {csv_file}: {e}")
        return pd.DataFrame(), False

def sanitize_total_jobs(df: pd.DataFrame,
                        preferred_col: Optional[str] = None) -> pd.DataFrame:
    """
    Ensure we have a numeric TOTAL_JOBS column.
    """
    candidate = None
    if preferred_col and preferred_col in df.columns:
        candidate = preferred_col
    elif 'TOTAL_JOBS' in df.columns:
        candidate = 'TOTAL_JOBS'
    else:
        # Try to guess a column with large ints
        for c in df.columns:
            if re.search(r'jobs|count|total', c.lower()):
                candidate = c
                break

    if candidate is None:
        logging.error("No suitable column found for TOTAL_JOBS.")
        df['TOTAL_JOBS'] = pd.Series(dtype='float')
        return df

    # Create/overwrite TOTAL_JOBS from candidate
    cleaned = (
        df[candidate]
        .astype(str)
        .str.replace(',', '', regex=False)
        .str.replace(' ', '', regex=False)
        .str.strip()
    )
    df['TOTAL_JOBS'] = pd.to_numeric(cleaned, errors='coerce')

    return df

# =============================================================
# Peak Calculation
# =============================================================

def latest_year_month(daily_df: pd.DataFrame) -> Tuple[int, int]:
    latest = daily_df['DATE'].max()
    return latest.year, latest.month

def filter_to_month(daily_df: pd.DataFrame,
                    year: Optional[int],
                    month: Optional[int]) -> pd.DataFrame:
    if year is None or month is None:
        year, month = latest_year_month(daily_df)
    result = daily_df[(daily_df.DATE.dt.year == year) & (daily_df.DATE.dt.month == month)].copy()
    return result, year, month

def compute_top_peaks(daily_df: pd.DataFrame,
                      top_n: int,
                      year: Optional[int],
                      month: Optional[int]) -> Tuple[pd.DataFrame, int, int]:
    month_df, y, m = filter_to_month(daily_df, year, month)
    if month_df.empty:
        return month_df, y, m
    try:
        peaks = month_df.nlargest(top_n, 'TOTAL_JOBS')
    except Exception:
        peaks = month_df.sort_values('TOTAL_JOBS', ascending=False).head(top_n)
    return peaks[['DATE', 'TOTAL_JOBS']], y, m

# =============================================================
# Chart Builders
# =============================================================

def peaks_transposed_chart(peaks_df: pd.DataFrame,
                           baseline: int,
                           chart_title: str) -> str:
    """
    Create chart with transposed data (dates as columns, series as rows)
    Using specific color codes: red #B83939 for baseline, green #829A3D for total jobs
    """
    if peaks_df.empty:
        return f"<p>No data for {chart_title}.</p>"
    
    # Sort by date
    peaks_df = peaks_df.sort_values('DATE')
    
    # Format dates as MM-DD for cleaner display
    date_format = peaks_df['DATE'].dt.strftime('%m-%d')
    
    # First row: Date headers
    header_row = "<tr><th>Date</th>"
    for date in date_format:
        header_row += f"<th>{date}</th>"
    header_row += "</tr>"
    
    # Second row: Baseline values
    baseline_row = "<tr><td>Baseline</td>"
    for _ in range(len(peaks_df)):
        baseline_row += f"<td>{int(baseline)}</td>"
    baseline_row += "</tr>"
    
    # Third row: Total Jobs values
    jobs_row = "<tr><td>Total Jobs</td>"
    for _, row in peaks_df.iterrows():
        jobs_row += f"<td>{int(row['TOTAL_JOBS'])}</td>"
    jobs_row += "</tr>"
    
    return f"""
<ac:structured-macro ac:name="chart">
  <ac:parameter ac:name="title">{chart_title}</ac:parameter>
  <ac:parameter ac:name="type">bar</ac:parameter>
  <ac:parameter ac:name="orientation">vertical</ac:parameter>
  <ac:parameter ac:name="3D">true</ac:parameter>
  <ac:parameter ac:name="width">600</ac:parameter>
  <ac:parameter ac:name="height">400</ac:parameter>
  <ac:parameter ac:name="legend">true</ac:parameter>
  <ac:parameter ac:name="dataDisplay">true</ac:parameter>
  <ac:parameter ac:name="stacked">false</ac:parameter>
  <ac:parameter ac:name="showValues">true</ac:parameter>
  <ac:parameter ac:name="colors">#B83939,#829A3D</ac:parameter>
  <ac:rich-text-body>
    <table>
      <tbody>
        {header_row}
        {baseline_row}
        {jobs_row}
      </tbody>
    </table>
  </ac:rich-text-body>
</ac:structured-macro>
"""

def peaks_detail_table(peaks_df: pd.DataFrame,
                       baseline: int,
                       label: str) -> str:
    if peaks_df.empty:
        return ""
    ranked = peaks_df.sort_values('TOTAL_JOBS', ascending=False).reset_index(drop=True)
    rank_map = {row.DATE: idx + 1 for idx, row in ranked.iterrows()}
    display = peaks_df.sort_values('DATE').copy()
    display['Variation'] = baseline - display['TOTAL_JOBS']
    rows = ["<tr><th>Rank</th><th>Date</th><th>Total Jobs</th><th>Baseline</th><th>Variation</th></tr>"]
    for _, r in display.iterrows():
        variation = int(r['Variation'])
        style = 'style="background-color:#90EE90;"' if variation > 0 else 'style="background-color:#FFB6C1;"'
        rows.append(
            f"<tr><td>{rank_map[r['DATE']]}</td>"
            f"<td>{r['DATE'].strftime('%Y-%m-%d')}</td>"
            f"<td>{int(r['TOTAL_JOBS'])}</td>"
            f"<td>{baseline}</td>"
            f"<td {style}>{variation}</td></tr>"
        )
    return f"<h4>Top 4 Peaks Detail ({label})</h4><table class='wrapped'><tbody>{''.join(rows)}</tbody></table>"

def generate_jobs_data_table(df: pd.DataFrame, baseline: int) -> str:
    """
    Generate the Jobs Data table showing ONLY TOP 4 PEAKS with Date, Baseline, Total Jobs, and Variation.
    FIXED: Variance = Baseline - Total Jobs (positive when baseline is higher, negative when total jobs are higher)
    """
    # Group by date and sum total jobs
    daily = df.groupby('DATE', as_index=False)['TOTAL_JOBS'].sum()
    
    if daily.empty:
        return "<p>No daily data available.</p>"
    
    # Get only top 4 peaks
    top_4_peaks = daily.nlargest(4, 'TOTAL_JOBS').sort_values('DATE')
    
    # Start the table
    html = """
<h3>Jobs Data - Top 4 Peaks</h3>
<table class='wrapped'>
<tbody>
<tr>
    <th>Date</th>
    <th>Baseline</th>
    <th>Total Jobs</th>
    <th>Variation</th>
</tr>
"""
    
    # Add data rows for top 4 only
    for _, row in top_4_peaks.iterrows():
        date_str = row['DATE'].strftime('%m/%d/%Y')
        total_jobs = int(row['TOTAL_JOBS'])
        variation = baseline - total_jobs  # FIXED: Baseline - Total Jobs as requested
        
        # Color coding logic - positive (baseline higher) = green, negative (total jobs higher) = red
        if variation > 0:
            var_style = 'style="background-color:#90EE90;"'  # Light green for positive
            var_text = f"{variation:,}"  # FIXED: Removed + sign
        elif variation < 0:
            var_style = 'style="background-color:#FFB6C1;"'  # Light red for negative
            var_text = f"{variation:,}"
        else:
            var_style = ''
            var_text = "0"
        
        html += f"""
<tr>
    <td>{date_str}</td>
    <td>{baseline:,}</td>
    <td>{total_jobs:,}</td>
    <td {var_style}>{var_text}</td>
</tr>
"""
    
    html += "</tbody></table>"
    return html

def generate_global_peaks_section(df: pd.DataFrame,
                                  baseline: int,
                                  top_n: int = 4) -> str:
    daily = df.groupby('DATE', as_index=False)['TOTAL_JOBS'].sum()
    peaks, y, m = compute_top_peaks(daily, top_n, None, None)
    if peaks.empty:
        return "<p>No peak data for current month.</p>"
    month_label = peaks['DATE'].dt.strftime('%b').iloc[0]
    chart = peaks_transposed_chart(peaks, baseline, f"Top {len(peaks)} Peaks of {month_label}")
    return chart

def generate_daily_total_single_series(df: pd.DataFrame,
                                       baseline: int,
                                       title: str = "Daily Total Jobs") -> str:
    """
    Generate daily totals chart with single green color and percentage labels.
    FIXED: Made X-axis labels vertical and increased chart height for better visibility
    """
    daily = df.groupby('DATE', as_index=False)['TOTAL_JOBS'].sum().sort_values('DATE')
    if daily.empty:
        return "<p>No daily data.</p>"
    
    # Convert to the format needed for the chart
    date_columns = {}
    for _, row in daily.iterrows():
        # Use DD-MM format for better readability and to avoid *** display
        date_str = row['DATE'].strftime('%d-%m')  # Changed to DD-MM format
        total_jobs = int(row['TOTAL_JOBS'])
        date_columns[date_str] = total_jobs
    
    # Create header row
    header_row = "<tr><th>Series</th>"
    for date in date_columns.keys():
        header_row += f"<th>{date}</th>"
    header_row += "</tr>"
    
    # Create data row
    data_row = "<tr><td>Total Jobs</td>"
    for value in date_columns.values():
        data_row += f"<td>{value}</td>"
    data_row += "</tr>"
    
    table_html = f"""<table><tbody>{header_row}{data_row}</tbody></table>"""
    
    chart = f"""
<h3>{title}</h3>
<ac:structured-macro ac:name="chart">
  <ac:parameter ac:name="type">bar</ac:parameter>
  <ac:parameter ac:name="title">{title}</ac:parameter>
  <ac:parameter ac:name="width">1200</ac:parameter>
  <ac:parameter ac:name="height">600</ac:parameter>
  <ac:parameter ac:name="3D">true</ac:parameter>
  <ac:parameter ac:name="legend">false</ac:parameter>
  <ac:parameter ac:name="orientation">vertical</ac:parameter>
  <ac:parameter ac:name="labelAngle">90</ac:parameter>
  <ac:parameter ac:name="stacked">false</ac:parameter>
  <ac:parameter ac:name="showValues">true</ac:parameter>
  <ac:parameter ac:name="xaxisLabel">Date</ac:parameter>
  <ac:parameter ac:name="yaxisLabel">Jobs</ac:parameter>
  <ac:parameter ac:name="colors">#829A3D</ac:parameter>
  <ac:parameter ac:name="dataDisplay">after</ac:parameter>
  <ac:parameter ac:name="fontSize">10</ac:parameter>
  <ac:rich-text-body>
    {table_html}
  </ac:rich-text-body>
</ac:structured-macro>
"""
    return chart

def generate_timeline_chart(df: pd.DataFrame,
                           baseline: int,
                           title: str = "Task Usage Timeline") -> str:
    """
    Generate timeline chart with separate lines per country if COUNTRY column exists.
    """
    # Check if we have country data
    if 'COUNTRY' in df.columns:
        # Multi-country timeline
        countries = df['COUNTRY'].unique()
        
        # Group by date and country
        country_data = df.groupby(['DATE', 'COUNTRY'], as_index=False)['TOTAL_JOBS'].sum()
        
        # Pivot to get countries as columns
        pivot_data = country_data.pivot(index='DATE', columns='COUNTRY', values='TOTAL_JOBS').fillna(0)
        pivot_data = pivot_data.sort_index()
        
        # Create table headers
        header_row = "<tr><th>Date</th>"
        for country in countries:
            header_row += f"<th>{country}</th>"
        header_row += "</tr>"
        
        # Create data rows
        data_rows = []
        for date, row in pivot_data.iterrows():
            date_str = date.strftime('%Y-%m-%d')
            data_row = f"<tr><td>{date_str}</td>"
            for country in countries:
                value = int(row[country]) if country in row else 0
                data_row += f"<td>{value}</td>"
            data_row += "</tr>"
            data_rows.append(data_row)
        
        table_html = f"""<table><tbody>{header_row}{''.join(data_rows)}</tbody></table>"""
        
        # Generate colors for each country
        colors = ["#829A3D", "#B83939", "#4472C4", "#E07C24", "#70AD47", "#9966CC"]
        country_colors = ",".join(colors[:len(countries)])
        
    else:
        # Single country/total timeline
        daily = df.groupby('DATE', as_index=False)['TOTAL_JOBS'].sum().sort_values('DATE')
        
        header_row = "<tr><th>Date</th><th>Total Jobs</th></tr>"
        
        data_rows = []
        for _, row in daily.iterrows():
            date_str = row['DATE'].strftime('%Y-%m-%d')
            total_jobs = int(row['TOTAL_JOBS'])
            data_rows.append(f"<tr><td>{date_str}</td><td>{total_jobs}</td></tr>")
        
        table_html = f"""<table><tbody>{header_row}{''.join(data_rows)}</tbody></table>"""
        country_colors = "#829A3D"
    
    if not table_html or "<tr><td>" not in table_html:
        return "<p>No timeline data available.</p>"
    
    chart = f"""
<h3>{title}</h3>
<ac:structured-macro ac:name="chart">
  <ac:parameter ac:name="type">line</ac:parameter>
  <ac:parameter ac:name="title">{title}</ac:parameter>
  <ac:parameter ac:name="width">1200</ac:parameter>
  <ac:parameter ac:name="height">500</ac:parameter>
  <ac:parameter ac:name="3D">false</ac:parameter>
  <ac:parameter ac:name="legend">true</ac:parameter>
  <ac:parameter ac:name="orientation">vertical</ac:parameter>
  <ac:parameter ac:name="stacked">false</ac:parameter>
  <ac:parameter ac:name="showValues">true</ac:parameter>
  <ac:parameter ac:name="xaxisLabel">Date</ac:parameter>
  <ac:parameter ac:name="yaxisLabel">Jobs</ac:parameter>
  <ac:parameter ac:name="colors">{country_colors}</ac:parameter>
  <ac:parameter ac:name="dataDisplay">after</ac:parameter>
  <ac:rich-text-body>
    {table_html}
  </ac:rich-text-body>
</ac:structured-macro>
"""
    return chart

# =============================================================
# Confluence REST Helpers
# =============================================================

def get_page_info(session: requests.Session, config: Dict) -> Tuple[Optional[str], Optional[int]]:
    try:
        base_url = config['CONFLUENCE_URL'].rstrip('/')
        if not base_url.endswith('/content'):
            if base_url.endswith('/rest/api'):
                base_url = base_url + "/content"
            else:
                base_url = base_url + "/rest/api/content"
        
        params = {
            'title': config['PAGE_TITLE'],
            'spaceKey': config['SPACE_KEY'],
            'expand': 'version'
        }
        resp = session.get(base_url, params=params)
        
        if resp.status_code != 200:
            alt = f"{base_url}/search?cql=space={config['SPACE_KEY']} AND title=\"{config['PAGE_TITLE']}\""
            resp = session.get(alt)
        
        if resp.status_code != 200:
            logging.error(f"Page lookup failed: {resp.status_code} - {resp.text}")
            return None, None
        
        data = resp.json()
        if 'results' in data and data.get('size', 0) > 0:
            return data['results'][0]['id'], data['results'][0]['version']['number']
        if isinstance(data, list) and data:
            return data[0]['id'], data[0]['version']['number']
        return None, None
    except Exception as e:
        logging.error(f"Error getting page info: {e}")
        return None, None

def create_or_update_page(session: requests.Session,
                          config: Dict,
                          content: str,
                          page_id: Optional[str] = None,
                          version: Optional[int] = None) -> bool:
    try:
        base_url = config['CONFLUENCE_URL'].rstrip('/')
        if not base_url.endswith('/content'):
            if base_url.endswith('/rest/api'):
                base_url = base_url + "/content"
            else:
                base_url = base_url + "/rest/api/content"
        
        payload = {
            "type": "page",
            "title": config['PAGE_TITLE'],
            "space": {"key": config['SPACE_KEY']},
            "body": {"storage": {"value": content, "representation": "storage"}}
        }
        
        if page_id and version is not None:
            payload["id"] = page_id
            payload["version"] = {"number": version + 1}
            resp = session.put(f"{base_url}/{page_id}", json=payload)
        else:
            resp = session.post(base_url, json=payload)
        
        if resp.status_code >= 400:
            logging.error(f"Create/Update failed: {resp.status_code} - {resp.text}")
            return False
        
        logging.info(f"Page '{config['PAGE_TITLE']}' {'updated' if page_id else 'created'} successfully.")
        return True
    except Exception as e:
        logging.error(f"Error creating/updating page: {e}")
        return False

def test_connection(session: requests.Session, config: Dict) -> bool:
    try:
        base = config['CONFLUENCE_URL'].rstrip('/')
        if base.endswith('/content'):
            base = base[:-8]
        elif not base.endswith('/rest/api'):
            base = base + '/rest/api'
        test_url = f"{base}/space"
        resp = session.get(test_url, params={'limit': 1})
        return resp.status_code == 200
    except Exception as e:
        logging.error(f"Connection test exception: {e}")
        return False

# =============================================================
# Main Publish Function
# =============================================================

def publish_to_confluence(report_file='task_usage_report_by_region.csv',
                          test_mode=False,
                          skip_actual_upload: bool = False) -> bool:
    """
    Publish report to Confluence with improved config file handling.
    """
    try:
        # Check report file exists
        if not os.path.exists(report_file):
            logging.error(f"Report file {report_file} not found.")
            return False
        
        # Find and load config
        cfg_path = find_config_file(test_mode)
        if not cfg_path:
            logging.error("Could not find config file")
            return False
        
        config = load_config(cfg_path)
        if not config:
            logging.error("Failed to load config")
            return False
        
        # Add test suffix if in test mode
        if test_mode and not config['PAGE_TITLE'].endswith("-TEST"):
            config['PAGE_TITLE'] += "-TEST"

        monthly = _is_monthly_run(config)
        baseline = config.get('BASELINE', 1899206)

        # Load and sanitize data
        df, ok = load_csv_data(report_file)
        if not ok:
            logging.error("Failed to load CSV data")
            return False
        
        df = sanitize_total_jobs(df, preferred_col=config.get('JOB_COLUMN'))

        # Create session if not skipping upload
        session = None
        if not skip_actual_upload:
            session = create_session(config)
            if not session:
                logging.error("Failed to create session")
                return False
            
            if not test_connection(session, config):
                logging.error("Connection test failed.")
                return False

        # Generate content sections
        peaks_section = generate_global_peaks_section(df, baseline) if monthly else ""
        jobs_data_table = generate_jobs_data_table(df, baseline) if monthly else ""
        daily_chart = generate_daily_total_single_series(df, baseline, "Daily Total Jobs")
        timeline_chart = generate_timeline_chart(df, baseline, "Task Usage Timeline")

        # Create page content
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        user = os.environ.get('USER', 'automated')
        
        content = f"""
<h1>Task Usage Report{' - TEST DATA' if test_mode else ''}</h1>
<p><strong>Last updated:</strong> {timestamp}</p>
<p><strong>Generated by:</strong> {user}</p>
<h2>Overall Monthly Task Usage Report</h2>
{jobs_data_table}
{peaks_section}
{daily_chart}
{timeline_chart}
<hr />
<p><em>Note: This report shows the task usage data{' (TEST MODE)' if test_mode else ''}.</em></p>
"""

        # Skip actual upload if requested
        if skip_actual_upload:
            logging.info("Skipping actual upload (simulation mode)")
            return True

        # Get page info and create/update
        page_id, version = get_page_info(session, config)
        return create_or_update_page(session, config, content, page_id, version)

    except Exception as e:
        logging.error(f"Error in publish_to_confluence: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return False

# =============================================================
# Multi-Country Publishing
# =============================================================

def publish_to_confluence_multi(report_files: List[Tuple[str, str]],
                                test_mode: bool = False,
                                skip_actual_upload: bool = False) -> bool:
    """
    Publish multi-country report to Confluence.
    """
    try:
        # Find and load config
        cfg_path = find_config_file(test_mode)
        if not cfg_path:
            logging.error("Could not find config file for multi-country")
            return False
        
        config = load_config(cfg_path)
        if not config:
            logging.error("Failed to load config for multi-country")
            return False
        
        if test_mode and not config['PAGE_TITLE'].endswith("-TEST"):
            config['PAGE_TITLE'] += "-TEST"

        monthly = _is_monthly_run(config)
        baseline = config.get('BASELINE', 1899206)

        # Process all country files
        frames = []
        for ctry, path in report_files:
            if not os.path.exists(path):
                logging.warning(f"Missing report for {ctry}: {path}")
                continue
            df = pd.read_csv(path)
            if 'DATE' in df.columns:
                df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')
            df = sanitize_total_jobs(df, preferred_col=config.get('JOB_COLUMN'))
            df['COUNTRY'] = ctry
            frames.append(df)

        if not frames:
            logging.error("No country data to publish.")
            return False

        all_df = pd.concat(frames, ignore_index=True)

        # Create session if not skipping
        session = None
        if not skip_actual_upload:
            session = create_session(config)
            if not session:
                return False
            if not test_connection(session, config):
                logging.error("Connection test failed.")
                return False

        # Generate content
        global_peaks = generate_global_peaks_section(all_df, baseline) if monthly else ""
        jobs_data_table = generate_jobs_data_table(all_df, baseline) if monthly else ""
        global_daily = generate_daily_total_single_series(all_df, baseline, "Daily Total Jobs (All Countries)")
        timeline_chart = generate_timeline_chart(all_df, baseline, "Task Usage Timeline (All Countries)")

        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        user = os.environ.get('USER', 'automated')
        
        content = f"""
<h1>Multi-Country Task Usage Report{' - TEST DATA' if test_mode else ''}</h1>
<p><strong>Last updated:</strong> {timestamp}</p>
<p><strong>Generated by:</strong> {user}</p>
<h2>Overall Monthly Task Usage Report</h2>
{jobs_data_table}
{global_peaks}
{global_daily}
{timeline_chart}
<hr />
<p><em>Note: This report shows aggregated data from multiple countries{' (TEST MODE)' if test_mode else ''}.</em></p>
"""

        if skip_actual_upload:
            logging.info("Simulation only (multi-country).")
            return True

        page_id, version = get_page_info(session, config)
        return create_or_update_page(session, config, content, page_id, version)

    except Exception as e:
        logging.error(f"Error in multi-country publish: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return False

# =============================================================
# CLI Entry Point
# =============================================================

if __name__ == "__main__":
    import argparse
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    
    p = argparse.ArgumentParser(description="Confluence Publisher")
    p.add_argument("--file", help="Single report CSV to publish")
    p.add_argument("--multi", nargs="*", help="List of COUNTRY=path/to/file.csv")
    p.add_argument("--simulate", action="store_true", help="Skip actual upload")
    p.add_argument("--test", action="store_true", help="Test mode")
    args = p.parse_args()
    
    if args.file:
        ok = publish_to_confluence(
            report_file=args.file,
            test_mode=args.test,
            skip_actual_upload=args.simulate
        )
        print("Single publish:", "OK" if ok else "FAILED")
    elif args.multi:
        pairs = []
        for item in args.multi:
            if "=" not in item:
                print(f"Invalid format: {item}")
                continue
            ctry, path = item.split("=", 1)
            pairs.append((ctry.strip(), path.strip()))
        ok = publish_to_confluence_multi(
            report_files=pairs,
            test_mode=args.test,
            skip_actual_upload=args.simulate
        )
        print("Multi publish:", "OK" if ok else "FAILED")
    else:
        p.print_help()
