#!/usr/bin/env python3
"""
Main workflow launcher - Enhanced with CSV auto-discovery.

Key features:
- Single or multi-country execution
- Date range modes: --monthly (previous month), --daily (YTD)
- Page title updates now target config.json (single source of truth)
- Query date range updates preserve the original query formatting (no column tampering)
- Test mode (--test) uses a predefined test CSV (handled inside workflow)
- --no-publish skips Confluence upload
- Multi-country support with simple date-based folder organization
- AUTO-DISCOVERY: Automatically detects CSV countries from files matching pattern
"""

import os
import sys
import json
import logging
import re
import calendar
from datetime import datetime, timedelta
import argparse
import tempfile
import shutil
import glob
import pandas as pd

# Make project root importable
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.append(PROJECT_ROOT)

from src.workflow import run_workflow  # noqa: E402
from lib.secure_config import SecureConfig  # noqa: E402
from lib.csv_processor import CSVProcessor  # noqa: E402
from lib.confluence_publisher import publish_to_confluence  # noqa: E402

# ---------------------------------------------------------------------------
# Constants / Configuration Paths
# ---------------------------------------------------------------------------

# Database / single-country INI (used ONLY for DB connection parameters)
CONFIG_FILE = 'config/config.ini'

# Authoritative Confluence configuration (page title lives here)
TITLE_CONFIG_FILE = 'config/config.json'

# Base SQL and default output
QUERY_FILE = 'config/query.sql'
OUTPUT_CSV = 'data.csv'

# Current execution metadata - UPDATED TO CURRENT TIME
EXECUTION_TIMESTAMP = datetime.strptime('2025-09-09 08:17:18', '%Y-%m-%d %H:%M:%S')
EXECUTION_USER = 'satish537'


# ---------------------------------------------------------------------------
# Logging
# ---------------------------------------------------------------------------

def setup_logging():
    log_file = f"workflow_{EXECUTION_TIMESTAMP.strftime('%Y%m%d_%H%M%S')}.log"
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )


# ---------------------------------------------------------------------------
# Credential Checks
# ---------------------------------------------------------------------------

def check_password_available():
    """
    Returns True if we can authenticate to Confluence (env password or encrypted JSON),
    otherwise False.
    """
    if os.environ.get('CONFLUENCE_PASSWORD'):
        return True
    
    # Check multiple possible locations for config.json
    config_paths = [
        "config/config.json",  # Most likely location based on your structure
        "config.json",         # Root directory fallback
        "config/config_test.json"  # Test config alternative
    ]
    
    for config_path in config_paths:
        abs_path = os.path.abspath(config_path)
        if os.path.exists(abs_path):
            try:
                with open(abs_path, 'r', encoding='utf-8') as f:
                    cfg = json.load(f)
                if 'PASSWORD_ENCRYPTED' in cfg and os.path.exists(SecureConfig.KEY_FILE):
                    return True
            except Exception:
                continue
    return False


# ---------------------------------------------------------------------------
# Date Range Helpers
# ---------------------------------------------------------------------------

def get_previous_month_date_range():
    """
    Return: (start_str, end_str, month_name) for previous calendar month.
    """
    today = datetime.now()
    first_day_current = datetime(today.year, today.month, 1)
    last_day_prev = first_day_current - timedelta(days=1)
    first_day_prev = datetime(last_day_prev.year, last_day_prev.month, 1)
    start_date_str = first_day_prev.strftime('%Y-%m-%d 00:00:00')
    end_date_str = last_day_prev.strftime('%Y-%m-%d 23:59:59')
    month_name = calendar.month_name[last_day_prev.month]
    logging.info(f"Previous month date range: {start_date_str} to {end_date_str} ({month_name})")
    return start_date_str, end_date_str, month_name


def get_ytd_date_range():
    """
    Year-to-date (Jan 1 .. today 23:59:59).
    """
    today = datetime.now()
    start_of_year = datetime(today.year, 1, 1)
    start_date_str = start_of_year.strftime('%Y-%m-%d 00:00:00')
    end_date_str = today.strftime('%Y-%m-%d 23:59:59')
    logging.info(f"YTD date range: {start_date_str} to {end_date_str}")
    return start_date_str, end_date_str


# ---------------------------------------------------------------------------
# Query Date Range Updater (Preserve Original Formatting)
# ---------------------------------------------------------------------------

def update_query_dates_preserve(query_path, start_date, end_date):
    """
    Preserve the original query.sql text; only update the BETWEEN date range.

    We look for the first occurrence of:
      BETWEEN TO_DATE('....', 'YYYY-MM-DD HH24:MI:SS') and TO_DATE('....', 'YYYY-MM-DD HH24:MI:SS')

    Replace only the two datetime literals. If pattern not found, log and return False.
    """
    import re as _re
    try:
        if not os.path.exists(query_path):
            logging.error("Query file not found: %s", query_path)
            return False

        with open(query_path, 'r', encoding='utf-8') as f:
            original = f.read()

        pattern = _re.compile(
            r"(BETWEEN\s+TO_DATE\(')"
            r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}"
            r"('\s*,\s*'YYYY-MM-DD HH24:MI:SS'\)\s+and\s+TO_DATE\(')"
            r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}"
            r"('\s*,\s*'YYYY-MM-DD HH24:MI:SS'\))",
            _re.IGNORECASE | _re.MULTILINE
        )

        def repl(m):
            return f"{m.group(1)}{start_date}{m.group(2)}{end_date}{m.group(3)}"

        new_query, count = pattern.subn(repl, original, count=1)
        if count == 0:
            logging.error("Could not locate BETWEEN date range to replace in %s", query_path)
            # Optional trace comment
            with open(query_path, 'a', encoding='utf-8') as f:
                f.write(f"\n-- WARNING: Date range replacement failed at {datetime.now()}\n")
            return False

        backup_path = f"{query_path}.bak"
        if not os.path.exists(backup_path):
            with open(backup_path, 'w', encoding='utf-8') as f:
                f.write(original)
            logging.info("Created backup at %s", backup_path)

        with open(query_path, 'w', encoding='utf-8') as f:
            f.write(new_query)

        logging.info(
            "Updated date range in %s to %s -> %s (preserved original formatting)",
            query_path, start_date, end_date
        )
        return True
    except Exception as e:
        logging.error("Error preserving query dates: %s", e)
        return False


# ---------------------------------------------------------------------------
# Page Title Updater (JSON Single Source) - UPDATED FOR YTD vs MONTH NAME
# ---------------------------------------------------------------------------

def update_confluence_page_title(config_path, suffix, is_daily=False):
    """
    Update PAGE_TITLE in JSON.
    
    For daily mode: append " - YTD" 
    For monthly mode: append " - <MonthName>" (e.g., " - August")
    """
    try:
        # Use absolute path to ensure we can find the config file
        abs_config_path = os.path.abspath(config_path)
        if not os.path.exists(abs_config_path):
            logging.error(f"Config file not found: {abs_config_path}")
            return False

        # JSON path (primary)
        if abs_config_path.endswith('.json'):
            try:
                with open(abs_config_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            except Exception as e:
                logging.error(f"Failed reading JSON config {abs_config_path}: {e}")
                return False

            if 'PAGE_TITLE' not in data:
                logging.error(f"PAGE_TITLE key not found in JSON {abs_config_path}")
                return False

            # Get base title by removing existing suffixes
            base = data['PAGE_TITLE']
            
            # Remove any existing suffixes (YTD, month names, _daily)
            base = re.sub(r'_daily$', '', base)
            base = re.sub(r'\s+-\s+YTD$', '', base)
            base = re.sub(
                r'\s+-\s+(January|February|March|April|May|June|July|August|September|October|November|December)$',
                '', base
            )
            
            # Apply new suffix based on mode
            if is_daily:
                new_title = f"{base} - YTD"
            else:
                # For monthly mode, append the month name (suffix parameter contains month name)
                new_title = f"{base} - {suffix}"

            data['PAGE_TITLE'] = new_title
            
            try:
                with open(abs_config_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=4)
            except Exception as e:
                logging.error(f"Failed writing JSON config {abs_config_path}: {e}")
                return False

            logging.info(f"Successfully updated Confluence PAGE_TITLE to: {new_title}")
            return True

        logging.error(f"Config file is not JSON format: {abs_config_path}")
        return False

    except Exception as e:
        logging.error(f"Error updating Confluence page title: {str(e)}")
        return False


# ---------------------------------------------------------------------------
# CSV File Analysis Helper
# ---------------------------------------------------------------------------

def analyze_csv_file(csv_path):
    """
    Analyze a CSV file and return information about its content.
    """
    try:
        if not os.path.exists(csv_path):
            return f"File does not exist: {csv_path}"
        
        file_size = os.path.getsize(csv_path)
        
        with open(csv_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        line_count = len(lines)
        data_rows = line_count - 1 if line_count > 0 else 0  # Subtract header row
        
        header = lines[0].strip() if lines else "No content"
        sample_data = lines[1].strip() if len(lines) > 1 else "No data rows"
        
        return f"Size: {file_size} bytes, Lines: {line_count}, Data rows: {data_rows}, Header: {header}, Sample: {sample_data}"
    except Exception as e:
        return f"Error analyzing {csv_path}: {e}"


# ---------------------------------------------------------------------------
# FIXED: CSV Auto-Discovery Functions (NO REGEX)
# ---------------------------------------------------------------------------

def auto_discover_csv_countries(csv_path, target_month):
    """
    Auto-discover CSV countries by scanning for: Countrycode_USAGEREPORT_YYYYMM.csv
    """
    if not os.path.exists(csv_path):
        logging.warning(f"CSV countries path does not exist: {csv_path}")
        return []
    
    # Simple pattern: MX_USAGEREPORT_202508.csv
    pattern = f"*_USAGEREPORT_{target_month}.csv"
    search_pattern = os.path.join(csv_path, pattern)
    logging.info(f"Scanning for CSV files: {search_pattern}")
    
    discovered_countries = []
    matching_files = glob.glob(search_pattern)
    
    for file_path in matching_files:
        filename = os.path.basename(file_path)
        if '_USAGEREPORT_' in filename:
            country_code = filename.split('_USAGEREPORT_')[0]
            if country_code and len(country_code) >= 2:
                discovered_countries.append({
                    'name': country_code,
                    'country_code': country_code,
                    'file_path': file_path,
                    'filename': filename
                })
                logging.info(f"Discovered CSV country: {country_code} -> {filename}")
    
    logging.info(f"Auto-discovered {len(discovered_countries)} CSV countries for {target_month}")
    return discovered_countries


def process_csv_file_countries(output_dir, target_month, test_mode=False):
    """
    Process countries that provide CSV files directly using auto-discovery.
    """
    # Load config to get CSV path
    try:
        with open('config/config.json', 'r') as f:
            config = json.load(f)
    except Exception as e:
        logging.warning(f"Could not load config for CSV countries: {e}")
        return []
    
    csv_path = config.get('CSV_COUNTRIES_PATH')
    if not csv_path:
        logging.info("No CSV_COUNTRIES_PATH configured, skipping CSV file countries")
        return []
    
    # Auto-discover CSV countries
    csv_countries = auto_discover_csv_countries(csv_path, target_month)
    
    if not csv_countries:
        logging.info(f"No CSV countries found for month {target_month}")
        return []
    
    logging.info(f"Processing {len(csv_countries)} auto-discovered CSV countries")
    
    results = []
    
    for csv_country in csv_countries:
        country_name = csv_country['name']
        country_code = csv_country['country_code']
        source_file_path = csv_country['file_path']
        
        # Output file in our processing directory
        output_file = os.path.join(output_dir, f"data_{country_name.lower()}.csv")
        
        logging.info(f"Processing CSV country: {country_name} from {source_file_path}")
        
        if test_mode:
            # Create dummy test data for CSV countries
            create_test_csv_country_data(output_file, country_name, target_month)
            results.append({
                'name': country_name,
                'csv_file': output_file,
                'success': True,
                'source': 'csv_file'
            })
            logging.info(f"Created test data for CSV country: {country_name}")
            continue
        
        try:
            # Process the actual CSV file
            success = process_single_csv_country_file(
                source_file_path, output_file, country_name, target_month
            )
            
            results.append({
                'name': country_name,
                'csv_file': output_file,
                'success': success,
                'source': 'csv_file'
            })
            
            if success:
                logging.info(f"Successfully processed CSV country: {country_name}")
            else:
                logging.error(f"Failed to process CSV country: {country_name}")
                
        except Exception as e:
            logging.error(f"Error processing CSV country {country_name}: {e}")
            results.append({
                'name': country_name,
                'csv_file': output_file,
                'success': False,
                'source': 'csv_file'
            })
    
    return results


def process_single_csv_country_file(source_file, output_file, country_name, target_month):
    """
    Process a single CSV file from a country and convert it to our standard format.
    FIXED: Properly aggregate daily totals from multiple CTM_HOST entries
    
    Expected input format: NET_DATE, CTM_HOST_NAME, FVALUE, JOBS (like your Excel screenshot)
    Output format: NET_DATE, TOTAL_JOBS (aggregated by date)
    """
    try:
        # Read the source CSV
        df = pd.read_csv(source_file)
        
        logging.info(f"Loaded CSV for {country_name}: {len(df)} rows")
        logging.info(f"Columns: {list(df.columns)}")
        
        # Log first few rows to understand the data
        logging.info(f"First 5 rows of raw data:")
        for i in range(min(5, len(df))):
            logging.info(f"  Row {i}: {df.iloc[i].to_dict()}")
        
        # Handle different possible column names for jobs
        jobs_column = None
        for col in ['JOBS', 'JOBS_PER_DAY', 'TOTAL_JOBS', 'JOB_COUNT', 'FVALUE']:
            if col in df.columns:
                jobs_column = col
                break
        
        if jobs_column is None:
            # If no jobs column found, assume the last numeric column
            numeric_cols = df.select_dtypes(include=['number']).columns
            if len(numeric_cols) > 0:
                jobs_column = numeric_cols[-1]
                logging.warning(f"No standard jobs column found, using: {jobs_column}")
            else:
                logging.error(f"No numeric column found for jobs in {country_name}")
                return False
        
        logging.info(f"Using jobs column: {jobs_column}")
        
        # Handle date column
        date_column = None
        for col in ['NET_DATE', 'DATE', 'DAY', 'REPORT_DATE']:
            if col in df.columns:
                date_column = col
                break
        
        if date_column is None:
            logging.error(f"No date column found in {country_name}")
            return False
        
        logging.info(f"Using date column: {date_column}")
        
        # Convert date column to datetime - handle various date formats
        # Try multiple date formats since Excel might export dates differently
        date_formats = [
            '%d/%m/%Y %H:%M',     # 01/08/2025 08:00
            '%d/%m/%Y %H:%M:%S',  # 01/08/2025 08:00:00
            '%Y-%m-%d %H:%M:%S',  # 2025-08-01 08:00:00
            '%Y-%m-%d',           # 2025-08-01
            '%m/%d/%Y %H:%M',     # 08/01/2025 08:00 (US format)
            '%m/%d/%Y %H:%M:%S',  # 08/01/2025 08:00:00 (US format)
        ]
        
        # Try to parse dates with different formats
        date_parsed = False
        for date_format in date_formats:
            try:
                df[date_column] = pd.to_datetime(df[date_column], format=date_format)
                date_parsed = True
                logging.info(f"Successfully parsed dates using format: {date_format}")
                break
            except:
                continue
        
        # If no format worked, try pandas auto-detection
        if not date_parsed:
            try:
                df[date_column] = pd.to_datetime(df[date_column], dayfirst=True)  # European format preference
                date_parsed = True
                logging.info("Successfully parsed dates using pandas auto-detection with dayfirst=True")
            except:
                try:
                    df[date_column] = pd.to_datetime(df[date_column])  # Let pandas figure it out
                    date_parsed = True
                    logging.info("Successfully parsed dates using pandas auto-detection")
                except Exception as e:
                    logging.error(f"Could not parse dates: {e}")
                    return False
        
        # Remove any rows where date parsing failed
        df = df[pd.notna(df[date_column])].copy()
        logging.info(f"After date parsing, {len(df)} valid rows remain")
        
        if df.empty:
            logging.error(f"No valid dates found after parsing in {country_name}")
            return False
        
        # Extract just the date part for grouping
        df['DATE_ONLY'] = df[date_column].dt.date
        
        # Log date range in the data
        min_date = df[date_column].min()
        max_date = df[date_column].max()
        logging.info(f"Date range in file: {min_date} to {max_date}")
        
        # Filter to target month if specified
        if target_month:
            target_year = int(target_month[:4])
            target_month_num = int(target_month[4:6])
            
            df_filtered = df[
                (df[date_column].dt.year == target_year) & 
                (df[date_column].dt.month == target_month_num)
            ].copy()
            
            if df_filtered.empty:
                logging.warning(f"No data found for target month {target_month} in {country_name}")
                # Use all data if no target month data
                df_filtered = df.copy()
            else:
                logging.info(f"Filtered to {len(df_filtered)} rows for month {target_month}")
        else:
            df_filtered = df.copy()
        
        # Convert jobs column to numeric, handling any formatting issues
        df_filtered[jobs_column] = pd.to_numeric(
            df_filtered[jobs_column].astype(str).str.replace(',', '').str.strip(), 
            errors='coerce'
        )
        
        # Remove rows with invalid job counts
        df_filtered = df_filtered[pd.notna(df_filtered[jobs_column])].copy()
        
        if df_filtered.empty:
            logging.error(f"No valid job counts found in {country_name}")
            return False
        
        # Group by date only (not datetime) and sum jobs - THIS IS THE KEY FIX
        daily_totals = df_filtered.groupby('DATE_ONLY')[jobs_column].sum().reset_index()
        daily_totals.columns = ['DATE_ONLY', 'TOTAL_JOBS']
        
        # Format date as datetime string for output (matching database format)
        daily_totals['NET_DATE'] = pd.to_datetime(daily_totals['DATE_ONLY']).dt.strftime('%Y-%m-%d %H:%M:%S')
        
        # Select only the columns we need
        daily_totals = daily_totals[['NET_DATE', 'TOTAL_JOBS']]
        
        # Sort by date
        daily_totals = daily_totals.sort_values('NET_DATE')
        
        # Log summary of what we're writing
        logging.info(f"Processed {country_name}: {len(daily_totals)} daily records")
        logging.info(f"Date range: {daily_totals['NET_DATE'].min()} to {daily_totals['NET_DATE'].max()}")
        logging.info(f"Total jobs: {daily_totals['TOTAL_JOBS'].sum():,}")
        
        # Log first few rows of output
        logging.info("First 5 rows of output:")
        for i in range(min(5, len(daily_totals))):
            row = daily_totals.iloc[i]
            logging.info(f"  {row['NET_DATE']}: {row['TOTAL_JOBS']:,.0f} jobs")
        
        # Save to output file
        daily_totals.to_csv(output_file, index=False)
        
        logging.info(f"Successfully wrote {len(daily_totals)} daily aggregated records to {output_file}")
        
        return True
        
    except Exception as e:
        logging.error(f"Error processing CSV file for {country_name}: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return False


def create_test_csv_country_data(output_file, country_name, target_month):
    """
    Create test data for CSV countries in test mode.
    """
    from datetime import datetime, timedelta
    
    # Parse target month
    target_year = int(target_month[:4])
    target_month_num = int(target_month[4:6])
    
    # Generate test data for the month
    start_date = datetime(target_year, target_month_num, 1)
    
    # Generate daily data for the month
    dates = []
    jobs = []
    
    for day in range(1, 32):  # Up to 31 days
        try:
            date = datetime(target_year, target_month_num, day)
            dates.append(date.strftime('%Y-%m-%d %H:%M:%S'))
            # Vary jobs by country
            base_jobs = {'MX': 3000, 'BR': 2500, 'CA': 1800}.get(country_name, 2000)
            daily_jobs = base_jobs + (day * 50) + (hash(country_name) % 500)
            jobs.append(daily_jobs)
        except ValueError:
            # Invalid date (e.g., Feb 31)
            break
    
    # Create DataFrame
    test_df = pd.DataFrame({
        'NET_DATE': dates,
        'TOTAL_JOBS': jobs
    })
    
    # Save test data
    test_df.to_csv(output_file, index=False)
    logging.info(f"Created test data for {country_name}: {len(test_df)} records")


# ---------------------------------------------------------------------------
# Enhanced Multi-Country Workflow Function
# ---------------------------------------------------------------------------

def run_workflow_multi_enhanced(countries, default_output_csv, execution_timestamp, execution_user, 
                               test_mode=False, publish_test=True, target_month=None):
    """
    Enhanced multi-country workflow that handles both database countries and CSV file countries.
    
    Args:
        countries: List of database countries from countries.json
        target_month: String in format 'YYYYMM' (e.g., '202509' for September 2025)
    """
    # If no target month specified, use current month
    if not target_month:
        target_month = datetime.now().strftime('%Y%m')
    
    logging.info(f"="*60)
    logging.info(f"Starting ENHANCED multi-country workflow for month: {target_month}")
    logging.info(f"Database countries: {[c['name'] for c in countries]}")
    logging.info(f"="*60)
    
    # Create date-based folder for organization
    today_folder = datetime.now().strftime('%Y-%m-%d')
    output_dir = os.path.join('reports', today_folder)
    os.makedirs(output_dir, exist_ok=True)
    
    original_cwd = os.getcwd()
    
    try:
        # Step 1: Process database countries (existing logic)
        country_results = []
        for country in countries:
            country_name = country['name']
            config_file = country['config_file']
            query_file = country['query_file']
            output_csv = os.path.join(output_dir, f"data_{country_name.lower()}.csv")
            
            logging.info(f"Processing database country: {country_name}")
            
            if test_mode:
                # Create dummy test data for database countries
                with open(output_csv, 'w') as f:
                    f.write("NET_DATE,TOTAL_JOBS\n")
                    for day in range(1, 31):
                        date_str = f"2025-09-{day:02d} 00:00:00"
                        jobs = 5000 + (day * 100)
                        f.write(f"{date_str},{jobs}\n")
                logging.info(f"Created test data for {country_name}")
            else:
                result = run_workflow(
                    config_file, query_file, output_csv,
                    execution_timestamp, execution_user,
                    test_mode=test_mode, publish_test=False
                )
                
                if result != 0:
                    logging.error(f"Failed to process database country {country_name}")
                    continue
            
            if os.path.exists(output_csv):
                country_results.append({
                    'name': country_name,
                    'csv_file': output_csv,
                    'success': True,
                    'source': 'database'
                })
                logging.info(f"Successfully processed database country: {country_name}")
        
        # Step 2: Process CSV file countries
        csv_countries_processed = process_csv_file_countries(
            output_dir, target_month, test_mode
        )
        
        # Add CSV countries to results
        country_results.extend(csv_countries_processed)
        
        # Step 3: Verify we have data to process
        successful_countries = [r for r in country_results if r['success']]
        if not successful_countries:
            logging.error("No countries processed successfully")
            return False
        
        # Log summary of data sources
        db_countries = [r for r in successful_countries if r['source'] == 'database']
        csv_countries = [r for r in successful_countries if r['source'] == 'csv_file']
        
        logging.info(f"Data sources summary:")
        logging.info(f"  Database countries: {len(db_countries)} - {[r['name'] for r in db_countries]}")
        logging.info(f"  CSV file countries: {len(csv_countries)} - {[r['name'] for r in csv_countries]}")
        
        # Step 4: Change to output directory and aggregate all data
        os.chdir(output_dir)
        
        csv_files = [f for f in os.listdir('.') if f.startswith('data_') and f.endswith('.csv')]
        logging.info(f"CSV files found for aggregation: {csv_files}")
        
        # Use CSV processor to aggregate all country data
        processor = CSVProcessor(execution_timestamp, execution_user)
        success = processor.process_all_files()
        
        if not success:
            logging.error("Failed to aggregate country data")
            return False
        
        # Step 5: Copy aggregated reports back and publish
        aggregated_report = "task_usage_report_by_region.csv"
        main_aggregated_path = os.path.join(original_cwd, aggregated_report)
        
        if os.path.exists(aggregated_report):
            shutil.copy2(aggregated_report, main_aggregated_path)
            logging.info(f"Copied aggregated report to {main_aggregated_path}")
        
        os.chdir(original_cwd)
        
        # Step 6: Publish to Confluence
        if publish_test:
            logging.info("Publishing aggregated report to Confluence...")
            publish_success = publish_to_confluence(
                report_file=aggregated_report,
                test_mode=test_mode,
                skip_actual_upload=False
            )
            if not publish_success:
                logging.error("Failed to publish to Confluence")
                return False
        
        # Final summary
        logging.info("="*60)
        logging.info("ENHANCED MULTI-COUNTRY WORKFLOW SUMMARY")
        logging.info("="*60)
        logging.info(f"Target Month: {target_month}")
        logging.info(f"Total Countries Processed: {len(successful_countries)}")
        logging.info(f"Database Countries: {len(db_countries)}")
        logging.info(f"CSV File Countries: {len(csv_countries)}")
        
        for result in successful_countries:
            status = "✓ SUCCESS"
            source_info = f"({result['source']})"
            logging.info(f"  {result['name']}: {status} {source_info}")
        
        return True
        
    except Exception as e:
        logging.error(f"Error in enhanced multi-country workflow: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return False
    finally:
        os.chdir(original_cwd)


# ---------------------------------------------------------------------------
# Multi-Country Workflow Function (Original)
# ---------------------------------------------------------------------------

def run_workflow_multi(countries, default_output_csv, execution_timestamp, execution_user, test_mode=False, publish_test=True):
    """
    Run workflow for multiple countries and aggregate results.
    Simple approach - create date folder and process countries there.
    """
    # Create date-based folder for organization
    today_folder = datetime.now().strftime('%Y-%m-%d')
    output_dir = os.path.join('reports', today_folder)
    os.makedirs(output_dir, exist_ok=True)
    
    logging.info(f"Created output directory: {output_dir}")
    
    # Store original working directory
    original_cwd = os.getcwd()
    
    try:
        # Process each country
        country_results = []
        for country in countries:
            country_name = country['name']
            config_file = country['config_file']
            query_file = country['query_file']  # This might be a temp file with updated dates
            output_csv = os.path.join(output_dir, f"data_{country_name.lower()}.csv")
            
            logging.info(f"Processing country: {country_name}")
            logging.info(f"Config: {config_file}")
            logging.info(f"Query: {query_file}")
            logging.info(f"Output: {output_csv}")
            
            # Run workflow for this country (stay in root directory)
            logging.info(f"About to call run_workflow for {country_name}")
            result = run_workflow(
                config_file,
                query_file,
                output_csv,
                execution_timestamp,
                execution_user,
                test_mode=test_mode,
                publish_test=False  # Don't publish individual countries
            )
            logging.info(f"run_workflow returned: {result} for {country_name}")
            
            # Analyze the generated CSV file
            csv_analysis = analyze_csv_file(output_csv)
            logging.info(f"CSV Analysis for {country_name}: {csv_analysis}")
            
            if result == 0:
                country_results.append({
                    'name': country_name,
                    'csv_file': output_csv,
                    'success': True
                })
                logging.info(f"Successfully processed {country_name}")
            else:
                country_results.append({
                    'name': country_name,
                    'csv_file': output_csv,
                    'success': False
                })
                logging.error(f"Failed to process {country_name}")
        
        # Check if any countries were processed successfully
        successful_countries = [r for r in country_results if r['success']]
        if not successful_countries:
            logging.error("No countries processed successfully")
            return False
        
        # Analyze all generated CSV files before aggregation
        logging.info("="*50)
        logging.info("INDIVIDUAL COUNTRY CSV ANALYSIS")
        logging.info("="*50)
        for result in successful_countries:
            csv_path = result['csv_file']
            analysis = analyze_csv_file(csv_path)
            logging.info(f"{result['name']}: {analysis}")
            
            # Show first few lines of each CSV file
            try:
                with open(csv_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()[:3]  # First 3 lines
                logging.info(f"First few lines of {result['name']} CSV:")
                for i, line in enumerate(lines):
                    logging.info(f"  Line {i+1}: {line.strip()}")
            except Exception as e:
                logging.error(f"Could not read {csv_path}: {e}")
        
        # Change to output directory for aggregation
        os.chdir(output_dir)
        logging.info(f"Changed to output directory: {os.getcwd()}")
        logging.info(f"Files in output directory: {os.listdir('.')}")
        
        # List all CSV files that will be processed
        csv_files = [f for f in os.listdir('.') if f.startswith('data_') and f.endswith('.csv')]
        logging.info(f"CSV files found for processing: {csv_files}")
        
        # Use the FIXED CSV processor
        logging.info("Using FIXED CSV processor...")
        processor = CSVProcessor(execution_timestamp, execution_user)
        success = processor.process_all_files()  # No parameters needed
        logging.info(f"Fixed CSV processor returned: {success}")
        
        if not success:
            logging.error("Failed to aggregate country data")
            return False
        
        # Analyze the generated aggregated reports
        aggregated_files = [
            "task_usage_report_by_region.csv",
            "task_usage_report.csv"
        ]
        
        logging.info("="*50)
        logging.info("AGGREGATED REPORTS ANALYSIS")
        logging.info("="*50)
        for report_file in aggregated_files:
            if os.path.exists(report_file):
                analysis = analyze_csv_file(report_file)
                logging.info(f"{report_file}: {analysis}")
                
                # Show first few lines of aggregated reports
                try:
                    with open(report_file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()[:5]  # First 5 lines
                    logging.info(f"First few lines of {report_file}:")
                    for i, line in enumerate(lines):
                        logging.info(f"  Line {i+1}: {line.strip()}")
                except Exception as e:
                    logging.error(f"Could not read {report_file}: {e}")
        
        # Copy aggregated reports to main directory for publishing
        aggregated_report = "task_usage_report_by_region.csv"
        detailed_report = "task_usage_report.csv"
        
        main_aggregated_path = os.path.join(original_cwd, aggregated_report)
        main_detailed_path = os.path.join(original_cwd, detailed_report)
        
        if os.path.exists(aggregated_report):
            shutil.copy2(aggregated_report, main_aggregated_path)
            logging.info(f"Copied aggregated report to {main_aggregated_path}")
        
        if os.path.exists(detailed_report):
            shutil.copy2(detailed_report, main_detailed_path)
            logging.info(f"Copied detailed report to {main_detailed_path}")
        
        # Change back to original directory for publishing
        os.chdir(original_cwd)
        
        # Publish aggregated report if not skipping
        if publish_test:
            logging.info("Publishing aggregated report to Confluence...")
            
            publish_success = publish_to_confluence(
                report_file=aggregated_report,
                test_mode=test_mode,
                skip_actual_upload=False
            )
            if not publish_success:
                logging.error("Failed to publish to Confluence")
                return False
        
        # Log summary of results
        logging.info("="*60)
        logging.info("MULTI-COUNTRY WORKFLOW SUMMARY")
        logging.info("="*60)
        logging.info(f"Output Directory: {output_dir}")
        logging.info(f"Total Countries: {len(country_results)}")
        logging.info(f"Successful: {len(successful_countries)}")
        logging.info(f"Failed: {len(country_results) - len(successful_countries)}")
        
        for result in country_results:
            status = "✓ SUCCESS" if result['success'] else "✗ FAILED"
            logging.info(f"  {result['name']}: {status}")
        
        if successful_countries:
            logging.info(f"Aggregated reports created and {'published' if publish_test else 'ready for publishing'}")
        
        logging.info("="*60)
        
        return len(successful_countries) > 0
        
    except Exception as e:
        logging.error(f"Error in multi-country workflow: {e}")
        return False
    finally:
        # Ensure we're back in the original directory
        os.chdir(original_cwd)


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(description="Enhanced data workflow with CSV auto-discovery")
    parser.add_argument("--test", action="store_true", help="Run in test mode using predefined test CSV")
    parser.add_argument("--no-publish", action="store_true", help="Skip actual publishing to Confluence")
    parser.add_argument("--monthly", action="store_true", help="Use previous month date range in SQL query")
    parser.add_argument("--daily", action="store_true", help="Use year-to-date range in SQL query")
    parser.add_argument("--countries-config", help="Path to countries JSON to run multi-country workflow")
    parser.add_argument("--target-month", help="Target month in YYYYMM format (e.g., 202509) - optional, defaults to current month")
    parser.add_argument("--csv-countries-only", action="store_true", help="Process only CSV file countries using auto-discovery")
    parser.add_argument("--list-csv-countries", action="store_true", help="List available CSV countries for target month")
    args = parser.parse_args()

    setup_logging()
    os.makedirs('config', exist_ok=True)

    # Determine target month with proper monthly logic
    if args.target_month:
        target_month = args.target_month
    elif args.monthly:
        # For monthly mode, use previous completed month (same as database logic)
        today = datetime.now()
        first_day_current = datetime(today.year, today.month, 1)
        last_day_prev = first_day_current - timedelta(days=1)
        target_month = last_day_prev.strftime('%Y%m')
        logging.info(f"Monthly mode: targeting previous completed month {target_month}")
    else:
        # Default to current month for daily or no-flag runs
        target_month = datetime.now().strftime('%Y%m')
        logging.info(f"Default mode: targeting current month {target_month}")

    # Handle CSV-only operations first
    if args.list_csv_countries:
        # Just list available CSV countries without processing
        try:
            with open('config/config.json', 'r') as f:
                config = json.load(f)
        except Exception as e:
            logging.error(f"Could not load config: {e}")
            return 1
        
        csv_path = config.get('CSV_COUNTRIES_PATH')
        if not csv_path:
            logging.error("No CSV_COUNTRIES_PATH configured")
            return 1
        
        csv_countries = auto_discover_csv_countries(csv_path, target_month)
        
        print(f"\nAvailable CSV countries for {target_month}:")
        print("=" * 50)
        if csv_countries:
            for country in csv_countries:
                print(f"  {country['country_code']}: {country['filename']}")
        else:
            print("  No CSV countries found")
        print()
        
        return 0

    if args.csv_countries_only:
        # Process only CSV file countries
        logging.info(f"Processing CSV file countries only for month {target_month}")
        
        output_dir = os.path.join('reports', datetime.now().strftime('%Y-%m-%d'))
        os.makedirs(output_dir, exist_ok=True)
        
        csv_results = process_csv_file_countries(output_dir, target_month, args.test)
        
        successful_csv = [r for r in csv_results if r['success']]
        
        if successful_csv:
            logging.info(f"Successfully processed {len(successful_csv)} CSV countries")
            for result in successful_csv:
                logging.info(f"  ✓ {result['name']}")
            return 0
        else:
            logging.error("No CSV countries processed successfully")
            return 1

    # Basic presence checks for single-country DB files if not test & not multi
    if not args.test and not args.countries_config:
        if not os.path.exists(CONFIG_FILE):
            logging.error(f"Config file {CONFIG_FILE} not found.")
            return 1
        if not os.path.exists(QUERY_FILE):
            logging.error(f"Query file {QUERY_FILE} not found.")
            return 1

    if args.monthly and args.daily:
        logging.error("Cannot use both --monthly and --daily flags together.")
        return 1

    date_range = None
    month_name = None
    if args.monthly:
        sd, ed, month_name = get_previous_month_date_range()
        date_range = (sd, ed, month_name)
    elif args.daily:
        sd, ed = get_ytd_date_range()
        date_range = (sd, ed, None)

    # Authentication check (unless skipping publish)
    if not args.no_publish and not check_password_available():
        print("No Confluence authentication credentials found.")
        print("1. Set CONFLUENCE_PASSWORD env var")
        print("2. Or run setup_secure_config.py to create encrypted config.json")
        print("3. Or use --no-publish to skip publishing")
        return 1

    # -----------------------------------------------------------------------
    # Multi-country Flow (Enhanced)
    # -----------------------------------------------------------------------
    if args.countries_config:
        try:
            with open(args.countries_config, "r", encoding='utf-8') as f:
                countries_cfg = json.load(f)
            countries = countries_cfg.get("countries", [])
            if not countries:
                logging.error(f"No countries defined in {args.countries_config}")
                return 1
        except Exception as e:
            logging.error(f"Failed to read countries config: {e}")
            return 1

        prepared = []
        for c in countries:
            name = c.get("name")
            cfg_path = c.get("config_file") or CONFIG_FILE
            qry_path = c.get("query_file") or QUERY_FILE
            if not name or not cfg_path:
                logging.error("Each country must have 'name' and 'config_file'")
                return 1
            if not args.test and not os.path.exists(cfg_path):
                logging.error(f"Config file for {name} not found: {cfg_path}")
                return 1
            if not args.test and not os.path.exists(qry_path):
                logging.error(f"Query file for {name} not found: {qry_path}")
                return 1

            # Handle date range updates if specified
            if date_range:
                sd, ed, _ = date_range
                tmp_qry = os.path.join(tempfile.gettempdir(), f"query_{name}.sql")
                # Copy original base query EXACTLY
                with open(qry_path, 'r', encoding='utf-8') as src, open(tmp_qry, 'w', encoding='utf-8') as dst:
                    dst.write(src.read())
                if not update_query_dates_preserve(tmp_qry, sd, ed):
                    logging.error(f"Failed to update date range for {name}")
                    return 1
                prepared.append({"name": name, "config_file": cfg_path, "query_file": tmp_qry})
            else:
                prepared.append({"name": name, "config_file": cfg_path, "query_file": qry_path})

        # Update title ONCE (global JSON) not per country
        if args.monthly and month_name:
            update_confluence_page_title(TITLE_CONFIG_FILE, month_name, is_daily=False)
        elif args.daily:
            update_confluence_page_title(TITLE_CONFIG_FILE, "YTD", is_daily=True)

        # Use enhanced workflow that handles both database and CSV countries
        result = run_workflow_multi_enhanced(
            countries=prepared,
            default_output_csv=OUTPUT_CSV,
            execution_timestamp=EXECUTION_TIMESTAMP,
            execution_user=EXECUTION_USER,
            test_mode=args.test,
            publish_test=not args.no_publish,
            target_month=target_month
        )
        return 0 if result else 1

    # -----------------------------------------------------------------------
    # Single-country Flow
    # -----------------------------------------------------------------------
    if args.monthly:
        sd, ed, month_name = date_range
        if not update_query_dates_preserve(QUERY_FILE, sd, ed):
            logging.error("Failed to update query dates for previous month")
            return 1
        if not update_confluence_page_title(TITLE_CONFIG_FILE, month_name, is_daily=False):
            logging.warning("Failed to update Confluence page title (JSON).")
        else:
            logging.info(f"Confluence page title updated to include: {month_name}")
        logging.info(f"Query date range updated (monthly) by {EXECUTION_USER}")

    elif args.daily:
        sd, ed, _ = date_range
        if not update_query_dates_preserve(QUERY_FILE, sd, ed):
            logging.error("Failed to update query dates for YTD")
            return 1
        if not update_confluence_page_title(TITLE_CONFIG_FILE, "YTD", is_daily=True):
            logging.warning("Failed to update Confluence page title (JSON).")
        else:
            logging.info("Confluence page title updated to include: YTD")
        logging.info(f"Query date range updated (daily/YTD) by {EXECUTION_USER}")

    # Run single workflow
    result = run_workflow(
        CONFIG_FILE,
        QUERY_FILE,
        OUTPUT_CSV,
        EXECUTION_TIMESTAMP,
        EXECUTION_USER,
        test_mode=args.test,
        publish_test=not args.no_publish
    )
    return result


# ---------------------------------------------------------------------------
# Entrypoint
# ---------------------------------------------------------------------------

if __name__ == "__main__":
    sys.exit(main())
