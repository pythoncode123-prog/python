import os
import csv
from datetime import datetime
import logging
from typing import Dict, List, Tuple, Optional

class CSVProcessor:
    """Class to handle CSV file processing and data aggregation."""

    DATE_FORMATS = [
        '%Y-%m-%d %H:%M:%S', '%Y-%m-%d',
        '%d-%m-%Y', '%Y/%m/%d', '%d/%m/%Y',
        '%d/%m/%y', '%m/%d/%Y', '%m/%d/%y',
        '%Y.%m.%d', '%d.%m.%Y', '%d.%m.%y',
        '%m.%d.%Y', '%m.%d.%y',
        '%d/%m/%Y', '%m/%d/%Y'  # Added common formats
    ]

    def __init__(self, execution_timestamp, execution_user):
        """Initialize the CSVProcessor."""
        self.execution_timestamp = execution_timestamp
        self.execution_user = execution_user
        self.summary_data: Dict[Tuple, int] = {}
        self.summary_data_region: Dict[Tuple, int] = {}

    @staticmethod
    def get_csv_files(directory: str) -> List[str]:
        """
        Get all CSV files in the specified directory that start with 'data_'.
        """
        return [file for file in os.listdir(directory) if file.endswith('.csv') and file.startswith('data_')]

    @staticmethod
    def parse_filename(filename: str) -> Tuple[Optional[str], Optional[str]]:
        """
        Extract region and environment from filename.
        For files like 'data_hk.csv', extract 'HK' as region.
        """
        if filename.startswith('data_'):
            # Extract region from data_hk.csv -> HK
            region = filename.replace('data_', '').replace('.csv', '').upper()
            return region, 'PROD'  # Default environment
        
        # Fallback to original logic
        parts = filename.split('-')
        if len(parts) >= 2:
            return parts[0], parts[1]
        return None, None

    def parse_date(self, net_date: str) -> Optional[datetime]:
        """
        Parse date string using multiple formats.
        """
        if not net_date or net_date == "DATE" or net_date == "REGION" or net_date == "NET_DATE":
            return None
            
        # Clean the date string
        net_date = net_date.strip()
        
        for fmt in self.DATE_FORMATS:
            try:
                return datetime.strptime(net_date, fmt)
            except ValueError:
                continue
        
        logging.warning(f"Could not parse date with any format: '{net_date}'")
        return None

    def process_line(self, line: List[str], region: str, env: str):
        """
        Process a single line of CSV data.
        """
        if not line or len(line) < 2:
            return
            
        # Skip header lines
        if any(header in str(line[0]).upper() for header in ['REGION', 'NET_DATE', 'DATE', 'TOTAL_JOBS']):
            return
        
        try:
            # Handle the format we see in your CSV: NET_DATE, TOTAL_JOBS
            if len(line) >= 2:
                date_str = line[0].strip()
                jobs_str = line[1].strip()
                
                # Parse date
                date_obj = self.parse_date(date_str)
                if date_obj is None:
                    logging.warning(f"Could not parse date: '{date_str}' in region {region}")
                    return
                    
                # Parse jobs count
                try:
                    jobs = int(jobs_str)
                except ValueError:
                    logging.warning(f"Could not parse jobs count: '{jobs_str}' in region {region}")
                    return
                
                date = date_obj.strftime('%Y-%m-%d')
                ctm_host_name = f'CTM_{region}'  # Generate a host name
                
                logging.debug(f"Processing: Region={region}, Env={env}, Date={date}, Jobs={jobs}")
                
                # Update summary dictionaries
                key = (region, env, date, ctm_host_name)
                key_region = (region, env, date)

                self.summary_data[key] = self.summary_data.get(key, 0) + jobs
                self.summary_data_region[key_region] = self.summary_data_region.get(key_region, 0) + jobs
                
        except Exception as e:
            logging.error(f"Error processing line {line} for region {region}: {str(e)}")

    def process_csv_file(self, file: str, default_csv_name: str = None) -> None:
        """
        Process a single CSV file.
        """
        logging.info(f"Processing file: {file}")
        
        # Extract region and environment from filename
        region, env = self.parse_filename(file)
        if not region:
            logging.warning(f"Could not extract region from filename: {file}")
            return
            
        logging.info(f"Extracted from {file}: Region={region}, Env={env}")
        
        try:
            with open(file, 'r', encoding='utf-8') as csv_file:
                csv_reader = csv.reader(csv_file)
                line_count = 0
                processed_count = 0
                
                for line in csv_reader:
                    line_count += 1
                    if line and len(line) >= 2:  # Must have at least date and jobs
                        # Skip empty or header lines
                        if line[0].strip() and not any(header in str(line[0]).upper() for header in ['NET_DATE', 'DATE', 'REGION']):
                            self.process_line(line, region, env)
                            processed_count += 1
                
                logging.info(f"File {file}: Read {line_count} lines, processed {processed_count} data lines")
                
        except Exception as e:
            logging.error(f"Error reading file {file}: {str(e)}")

    def write_summary_to_csv(self, filename: str) -> bool:
        """
        Write summary data to CSV file.
        """
        try:
            logging.info(f"Writing summary data to {filename}")
            logging.info(f"Summary data contains {len(self.summary_data)} entries")
            
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile)
                # Write header
                writer.writerow(['REGION', 'ENV', 'DATE', 'CTM_HOST_NAME', 'TOTAL_JOBS', 'EXECUTION_TIMESTAMP', 'EXECUTION_USER'])
                
                rows_written = 0
                # Write data
                for (region, env, date, ctm_host_name), total_jobs in self.summary_data.items():
                    writer.writerow([
                        region, 
                        env, 
                        date, 
                        ctm_host_name, 
                        total_jobs,
                        self.execution_timestamp.strftime('%Y-%m-%d %H:%M:%S'),
                        self.execution_user
                    ])
                    rows_written += 1
                
                logging.info(f"Successfully wrote {rows_written} rows to {filename}")
                return True
                
        except Exception as e:
            logging.error(f"Error writing summary to {filename}: {str(e)}")
            return False

    def write_summary_to_csv_by_region(self, filename: str) -> bool:
        """
        Write region summary data to CSV file.
        """
        try:
            logging.info(f"Writing region summary data to {filename}")
            logging.info(f"Region summary data contains {len(self.summary_data_region)} entries")
            
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile)
                # Write header
                writer.writerow(['REGION', 'ENV', 'DATE', 'TOTAL_JOBS', 'EXECUTION_TIMESTAMP', 'EXECUTION_USER'])
                
                rows_written = 0
                # Write data
                for (region, env, date), total_jobs in self.summary_data_region.items():
                    writer.writerow([
                        region, 
                        env, 
                        date, 
                        total_jobs,
                        self.execution_timestamp.strftime('%Y-%m-%d %H:%M:%S'),
                        self.execution_user
                    ])
                    rows_written += 1
                
                logging.info(f"Successfully wrote {rows_written} rows to {filename}")
                return True
                
        except Exception as e:
            logging.error(f"Error writing region summary to {filename}: {str(e)}")
            return False

    def process_all_files(self, output_path: str = None) -> bool:
        """
        Process all CSV files in current directory and generate reports.
        
        Args:
            output_path: Optional output path (for backward compatibility)
        """
        try:
            logging.info("Starting CSV processing...")
            
            # Get current directory
            current_dir = os.getcwd()
            logging.info(f"Processing files in directory: {current_dir}")
            
            # Get all CSV files that start with 'data_'
            csv_files = self.get_csv_files(current_dir)
            logging.info(f"Found CSV files: {csv_files}")
            
            if not csv_files:
                logging.warning("No data CSV files found for processing")
                return False
            
            # Process each file
            for csv_file in csv_files:
                self.process_csv_file(csv_file)
            
            logging.info(f"Processed {len(csv_files)} CSV files")
            logging.info(f"Total summary entries: {len(self.summary_data)}")
            logging.info(f"Total region summary entries: {len(self.summary_data_region)}")
            
            # Write summary files
            summary_success = self.write_summary_to_csv('task_usage_report.csv')
            region_success = self.write_summary_to_csv_by_region('task_usage_report_by_region.csv')
            
            if summary_success and region_success:
                logging.info("CSV processing completed successfully")
                return True
            else:
                logging.error("Failed to write one or more summary files")
                return False
                
        except Exception as e:
            logging.error(f"Error in process_all_files: {str(e)}")
            return False
